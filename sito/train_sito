#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.

import os, sys
import argparse
from typing import Optional, Any
from dataclasses import dataclass
import logging.config

import decimal
import math
import random

#import numpy
import torch
import torch.nn as nn
import torchaudio
import torchaudio.sox_effects

import csv
import yaml, json
from omegaconf import OmegaConf

import sentencepiece as spm
#import k2

from py.model.lstm import Model

# Global Constants
G_FEATURE_PADDING_VALUE = float(0.0)
G_LABEL_PADDING_VALUE = int(-1)
G_BLANK_ID = 0
G_BOS_ID = 1
G_EOS_ID = 2

G_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

@dataclass
class Sample:
    id: str = ''
    audio: str = ''
    begin: float = 0.0
    duration: float = 0.0
    text: str = ''
    speaker: str = ''


class SampleLoader:
    ''' A callable wrapper class to load one dataset sample from one utterance
    input:
        utt dict is typically yielded by .tsv .jsonl .yaml file reader.

    return:
        a Sample object or None
        None if an utterance doesn't satisfy constraints e.g. duration, text length
    '''
    def __init__(self, field_map:dict, min_duration:float, max_duration:float, min_text_length:int = 1, max_text_length:int = 2048, data_root: str = ''):
        self.field_map = field_map

        self.min_duration = min_duration
        self.max_duration = max_duration
        self.min_text_length = min_text_length
        self.max_text_length = max_text_length

        self.data_root = data_root
    
    def __call__(self, utt: dict) -> Optional[Sample] :
        sample = Sample()
        for attr, field in self.field_map.items():
            if v := utt.get(field):
                if attr in ['duration', 'begin', ]:
                    v = float(v)
                elif attr == 'audio':
                    v = os.path.join(self.data_root, v)
                else:
                    v = str(v)
                assert hasattr(sample, attr), f'{field} -> Sample.{attr} mapping failed, no such attribute'
                setattr(sample, attr, v)

        if sample.duration < self.min_duration:
            return None
        if sample.duration > self.max_duration:
            return None
        if len(sample.text) < self.min_text_length:
            return None
        if len(sample.text) > self.max_text_length:
            return None

        return sample


class Dataset:
    def __init__(self, data_zoo_config, dataset_config, sample_loader_config) :
        self.samples = []
        data_zoo = OmegaConf.load(data_zoo_config)

        for subset in dataset_config.subsets:
            if subset.max_num_samples == 0:
                continue
            elif subset.max_num_samples < 0:
                subset.max_num_samples = sys.maxsize # https://docs.python.org/3/library/sys.html#sys.maxsize

            # retrive dataset info from data zoo
            subset_root, subset_meta = data_zoo[subset.id].root, data_zoo[subset.id].metadata
            logging.info(f'Loading {subset.id} from ({subset_root} : {subset_meta}) ...')
            with open(subset_meta, 'r', encoding='utf8') as f:
                if str.endswith(subset_meta, '.tsv'):
                    utterance_reader = csv.DictReader(f, delimiter='\t')
                else:
                    raise NotImplementedError

                k = 0
                sample_loader = SampleLoader(**sample_loader_config, data_root = subset_root)
                for utt in utterance_reader:
                    if k >= subset.max_num_samples:
                        break
                    if sample := sample_loader(utt):
                        self.samples.append(sample)
                        k += 1
                logging.info(f'{k} samples loaded from {subset.id}')
        logging.info(F'Total {len(self.samples)} loaded.')
        # length sort

        # shuffle
             
    def __getitem__(self, index: int):
        return self.samples[index]
    
    def __len__(self):
        return len(self.samples)


# https://github.com/lhotse-speech/lhotse/blob/master/lhotse/utils.py#L367 : compute_num_samples()
def seconds_to_samples(sample_rate:int, seconds:float):
    """
    Convert a time quantity to the number of samples given a specific sampling rate.
    Performs consistent rounding up or down for ``duration`` that is not a multiply of
    the sampling interval (unlike Python's built-in ``round()`` that implements banker's rounding).
    """
    return int(
        decimal.Decimal(
            round(seconds * sample_rate, ndigits=8)
        ).quantize(0, rounding=decimal.ROUND_HALF_UP)
    )


def load_audio(audio_path: str, begin: float, duration: float) :
    '''
    https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-i-o

    torchaudio.info(sample.audio)
    -> 
    AudioMetaData(
        sample_rate=16000, num_frames=31872, num_channels=1, bits_per_sample=16, encoding=PCM_S,
    )
    '''
    info = torchaudio.info(audio_path)
    waveform, sample_rate = torchaudio.load(
        audio_path,
        frame_offset = seconds_to_samples(info.sample_rate, begin),
        num_frames   = seconds_to_samples(info.sample_rate, duration),
    )
    return waveform, sample_rate


class Resampler:
    def __init__(self, sample_rate: int):
        self.target_sample_rate = sample_rate

    def __call__(self, waveform: torch.Tensor, sample_rate: int):
        if sample_rate != self.target_sample_rate:
            waveform = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)(waveform)
        return waveform, self.target_sample_rate


class Perturbation:
    def __init__(self, 
        mark_key: bool = False,
        speeds: Optional[list[float]] = None, 
        tempos: Optional[list[float]] = None, 
        volumes: Optional[list[float]] = None,
    ) :
        self.mark_key = mark_key
        self.speeds = speeds
        self.tempos = tempos
        self.volumes = volumes

    def __call__(self, key: str, waveform: torch.Tensor, sample_rate: int):
        '''
        Note: Regarding to 'speed' and 'tempo' perturbation:

        in classic SoX CLI:
            soxi raw.wav -> sample_rate: 16000, duration: 00:00:01.99 = 31872 samples

            sox raw.wav speed.wav speed 1.5
            soxi speed.wav -> sample_rate: 16000, 00:00:01.33 = 21248 sample

            sox raw.wav tempo.wav tempo 1.5
            soxi tempo.wav -> sample_rate: 16000, 00:00:01.33 = 21248 sample

        whereas in torchaudio.sox_effect function calls:
            output waveform sample_rate is changed,
            so here we need explicitly 'rate' resampling after 'speed' or 'tempo'

        '''
        effects_chain = []
        if self.speeds:
            if (factor := random.choice(self.speeds)) != 1.0:
                effects_chain.extend([
                    ['speed', str(factor)],
                    ['rate', str(sample_rate)]
                ])
                if self.mark_key:
                    key = f'{key}__speed{factor}'
        
        if self.tempos:
            if (factor := random.choice(self.tempos)) != 1.0:
                effects_chain.extend([
                    ['tempo', str(factor)],
                    ['rate', str(sample_rate)]
                ])
                if self.mark_key:
                    key = f'{key}__tempo{factor}'

        if self.volumes:
            if (factor := random.choice(self.volumes)) != 1.0:
                effects_chain.append(['vol', str(factor)])
                if self.mark_key:
                    key = f'{key}__vol{factor}'

        if effects_chain:
            new_waveform, new_sample_rate = torchaudio.sox_effects.apply_effects_tensor(
                waveform, sample_rate,
                effects_chain,
            )
            assert new_sample_rate == sample_rate # perturbation shouldn't change sample_rate
            waveform, sample_rate = new_waveform, new_sample_rate

        return key, waveform, sample_rate


class FeatureExtractor:
    def __init__(self, feature_extractors_config:dict, feature_type:str, ):
        if feature_type == 'fbank':
            self.extractor = FbankFeatureExtractor(**feature_extractors_config['fbank'])
        elif feature_type == 'hires_mfcc':
            raise NotImplementedError(f'feature_type:{feature_type} is not supported yet')
        else:
            raise RuntimeError(f'Unknown feature_type:{feature_type}')
            
    def __call__(self, waveform: torch.Tensor, sample_rate: int):
        return self.extractor(waveform, sample_rate)


class FbankFeatureExtractor:
    def __init__(self, num_mel_bins:int, dither:float = 0.0):
        self.num_mel_bins = num_mel_bins
        self.dither = dither

    def __call__(self, waveform: torch.Tensor, sample_rate: int):
        '''
        kaldi:
            https://github.com/kaldi-asr/kaldi/blob/master/src/feat/mel-computations.h#L60
            https://github.com/kaldi-asr/kaldi/blob/master/src/feat/feature-fbank.h#L62

        torchaudio:
            https://pytorch.org/audio/stable/compliance.kaldi.html
        '''
        # By default, torchaudio uses normalized 32bits float waveform [-1.0, 1.0]
        # To use Kaldi compliance function, waveform needs to be convert to 16bits signed-interger PCM [-32768, 32767]
        # TODO:
        # torchaudio's feature extraction is not consistent with Kaldi https://github.com/pytorch/audio/issues/400
        # torchaudio official plan to bind kaldi feat lib: https://github.com/pytorch/audio/issues/1269
        # to not finished up to 2021-10-02 : https://github.com/pytorch/audio/pull/1326
        # keep an eye on this work and do it myself if necessary
        assert waveform.dtype == torch.float32
        INT16PCM_waveform = waveform * torch.iinfo(torch.int16).max
        feature = torchaudio.compliance.kaldi.fbank(
            INT16PCM_waveform,
            sample_frequency = sample_rate,
            num_mel_bins = self.num_mel_bins,
            dither = self.dither,
            energy_floor = torch.finfo(torch.float).eps,
            # about energy_floor:
            #   Kaldi default = 0.0; torchaudio default = 1.0;
            #   it doesn't matter if use_energy = False (default in both)
        )

        return feature

class MeanVarStats:
    def __init__(self):
        self.o1_stats = None
        self.o2_stats = None
        self.n = 0

    def __repr__(self):
        return (
            f'\n'
            f'Global MEAN/VAR stats:\n'
            f'    1st_order:  {self.o1_stats}\n'
            f'    2nd_order:  {self.o2_stats}\n'
            f'    frames:   {self.n}\n'
        )

    def accumulate_mean_var_stats(self, feature:torch.Tensor):
        num_frames, feature_dim = feature.shape
        if self.n == 0:
            self.o1_stats = torch.zeros(feature_dim)
            self.o2_stats = torch.zeros(feature_dim)

        assert self.o1_stats.shape[0] == feature_dim, f'mvn accumulator dim {self.o1_stats.shape[0]} inconsistent with feature dim {feature_dim}'
        self.o1_stats += feature.sum(dim=0)
        self.o2_stats += feature.square().sum(dim=0)
        self.n += num_frames

    def load(self, input_file:str):
        with open(input_file, 'r') as f:
            stats = json.load(input_file)
            self.o1_stats = torch.Tensor(stats['o1_stats'])
            self.o2_stats = torch.Tensor(stats['o2_stats'])
            self.n = stats['n']

    def dump(self, output_file:str):
        stats = {
            'o1_stats': self.o1_stats.tolist(),
            'o2_stats': self.o2_stats.tolist(),
            'n': self.n,
        }
        with open(output_file, 'w+') as f:
            json.dump(stats, f, indent=4)

class MeanVarNormalizer:
    def __init__(self, mean_var_stats:MeanVarStats):
        mean = mean_var_stats.o1_stats / mean_var_stats.n
        var  = mean_var_stats.o2_stats / mean_var_stats.n - mean.square()
        self.mean_norm_shift = -mean
        self.var_norm_scale = var.sqrt().clamp(min = torch.finfo(torch.float32).eps).reciprocal()

    def __repr__(self):
        return (
            f'\n'
            f'Global MEAN/VAR normalizer:\n'
            f'    mean_shift:  {self.mean_norm_shift}\n'
            f'    var_scale:  {self.var_norm_scale}\n'
        )
    
    def __call__(self, feature:torch.Tensor):
        feature += self.mean_norm_shift
        feature *= self.var_norm_scale
        return feature


class Tokenizer:
    def __init__(self, model: str):
        self.sentence_piece = spm.SentencePieceProcessor()
        self.sentence_piece.load(model)
        self.vocab = [ [ self.sentence_piece.id_to_piece(x), x] for x in range(self.sentence_piece.get_piece_size())]
    
    def __call__(self, text) -> tuple[list[str], list[int]] :
        token_pieces = self.sentence_piece.encode_as_pieces(text)
        token_ids = self.sentence_piece.encode_as_ids(text)
        return token_pieces, token_ids


class TextNormalizer:
    def __init__(self, case: str):
        self.case = case
    
    def __call__(self, text):
        if self.case == 'UPPER':
            text = text.upper()
        elif self.case == 'lower':
            text = text.lower()
        return text


class DataPipeline:
    def __init__(
        self,
        resampler : Optional[callable] = None,
        perturbation:Optional[callable] = None,
        feature_extractor : Optional[callable] = None,
        mean_var_normalizer : Optional[callable] = None,
        text_normalizer: Optional[callable] = None,
        tokenizer: Optional[callable] = None,
    ) :
        self.resampler = resampler
        self.perturbation = perturbation
        self.feature_extractor = feature_extractor
        self.mean_var_normalizer = mean_var_normalizer
        self.text_normalizer = text_normalizer
        self.tokenizer = tokenizer

    def __call__(self, samples: list[Sample]):
        minibatch = []
        for sample in samples:
            # raw audio loading
            key = sample.id
            waveform, sample_rate = load_audio(sample.audio, sample.begin, sample.duration)

            if self.resampler:
                waveform, sample_rate = self.resampler(waveform, sample_rate)

            if self.perturbation:
                key, waveform, sample_rate = self.perturbation(key, waveform, sample_rate)

            # reverb

            # add noise
            
            # feature extraction
            feature = torch.Tensor([])
            if self.feature_extractor:
                feature = self.feature_extractor(waveform, sample_rate)
            
            if self.mean_var_normalizer:
                feature = self.mean_var_normalizer(feature)

            # specaug
            
            # detach
            feature = feature.detach()

            text = sample.text
            # text normalization
            if self.text_normalizer:
                text = self.text_normalizer(text)

            # tokenization
            token_pieces, token_ids = [], []
            if self.tokenizer:
                token_pieces, token_ids = self.tokenizer(text)

            # store everything here for debug purpose
            minibatch.append({
                'key': key,
                'waveform': waveform,
                'sample_rate': sample_rate,
                'feature': feature,
                'text': text,
                'token_pieces': token_pieces,
                'token_ids': token_ids,
            })
            logging.debug(f'Processed sample {sample.id} -> {key}')

        features = [ x['feature'] for x in minibatch ]
        time_lengths = torch.tensor([ len(x) for x in features ]) # [B]
        inputs = nn.utils.rnn.pad_sequence(
            features,
            batch_first = True,
            padding_value = G_FEATURE_PADDING_VALUE,
        ) # [B, max(time_lengths), F]

        labels = [ x['token_ids'] for x in minibatch ]
        label_lengths = torch.tensor([ len(y) for y in labels ]) # [B]
        targets = nn.utils.rnn.pad_sequence(
            [ torch.tensor(l) for l in labels ],
            batch_first = True,
            padding_value = G_LABEL_PADDING_VALUE,
        ) # [B, max(label_lengths)]

        return minibatch, inputs, targets, time_lengths, label_lengths


def Loss(log_probs, labels, input_lens, label_lens, blank_index = G_BLANK_ID):
    log_probs = torch.einsum("btv->tbv", log_probs)
    loss = torch.nn.functional.ctc_loss(
        log_probs,
        labels,
        input_lens,
        label_lens,
        blank_index,
        zero_infinity=True,
        reduction='sum',
    )
    return loss


def make_length_mask(lengths: torch.Tensor) -> torch.Tensor:
    """Make mask tensor indicating in-length elements.
    Args:
        lengths (torch.Tensor): Batch of lengths (B,).
    Returns:
        torch.Tensor: Bool tensor mask indicating in-length elements.
    """
    B = int(lengths.size(0))
    L = int(lengths.max().item())
    range_tensor = torch.arange(0, L, dtype=torch.int64, device=lengths.device).unsqueeze(0).expand(B, L)
    mask = range_tensor < lengths.unsqueeze(-1)
    return mask


def ctc_collapse(hyp: list[int]) -> list[int]:
    new_hyp: list[int] = []
    cur = 0
    while cur < len(hyp):
        if hyp[cur] != G_BLANK_ID:
            new_hyp.append(hyp[cur])
        prev = cur
        while cur < len(hyp) and hyp[cur] == hyp[prev]:
            cur += 1
    return new_hyp


def ctc_greedy_search(log_probs: torch.Tensor, time_lengths: torch.Tensor):
    B = log_probs.shape[0]

    best_scores, best_paths = log_probs.topk(1, dim=2) # [B,T,V] -> [B,T,k=1]

    best_paths = best_paths.squeeze(2)
    best_scores = best_scores.squeeze(2)
    pad_mask = ~ make_length_mask(time_lengths)

    best_paths.masked_fill_(pad_mask, G_EOS_ID)
    best_scores.masked_fill_(pad_mask, 0.0)
    scores = best_scores.sum(1)
    hyps = [ ctc_collapse(p.tolist()) for p in best_paths ]
    #hyps = [ p.tolist() for p in best_paths ]
    return hyps, scores


def dump_tensor_to_csv(x: torch.Tensor, filename: str):
    import pandas as pd
    df = pd.DataFrame(x.numpy())
    df.to_csv(filename)


def move_tensor_to_device(*tensors):
    return tuple([t.to(G_DEVICE) for t in tensors])


def compute_mean_var_stats(dataset, config):
    feature_pipeline = DataPipeline(
        resampler = Resampler(**config.resampler),
        perturbation = Perturbation(**config.perturbation),
        feature_extractor = FeatureExtractor(config.feature_extractors, config.feature_type),
    )

    # data loaders
    dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle = False,
        batch_size = 32, 
        drop_last = False,
        num_workers = 4,
        collate_fn = feature_pipeline,
    )

    logging.info('Computing global mean variance stats ...')
    stats = MeanVarStats()
    for batch in dataloader:
        samples, _,_,_,_ = batch
        for sample in samples:
            stats.accumulate_mean_var_stats(sample['feature'])  # [Time, Freq]
    return stats


def run():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type = str, required = True)
    args = parser.parse_args()

    config = OmegaConf.load(args.config)
    print(OmegaConf.to_yaml(config), file = sys.stderr, flush = True)

    # setup logging
    if os.path.isfile(config.logging):
        with open(config.logging, 'r', encoding='utf8') as f:
            logging.config.dictConfig(yaml.safe_load(f))
    else:
        logging.basicConfig(
            stream=sys.stderr, level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s'
        )

    # load train/dev/test dataset from data zoo
    train_dataset = Dataset(config.data_zoo, config.dataset.train, config.sample_loader)
    valid_dataset = Dataset(config.data_zoo, config.dataset.valid, config.sample_loader)

    # compute feature mean & var normalizer
    mean_var_stats = compute_mean_var_stats(train_dataset, config)
    mean_var_stats.dump('mean_var_stats.json')

    # assemble data processing pipline
    tokenizer = Tokenizer(**config.tokenizer)
    mvn = MeanVarNormalizer(mean_var_stats)
    logging.info(mvn)
    data_pipeline = DataPipeline(
        resampler = Resampler(**config.resampler),
        perturbation = Perturbation(**config.perturbation),
        feature_extractor = FeatureExtractor(config.feature_extractors, config.feature_type),
        mean_var_normalizer = mvn,
        text_normalizer = TextNormalizer(**config.text_normalizer),
        tokenizer = tokenizer,
    )

    # data loaders
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, 
        **config.dataloader,
        collate_fn = data_pipeline,
    )

    valid_dataloader = torch.utils.data.DataLoader(
        valid_dataset, 
        shuffle = False,
        batch_size = config.dataloader.batch_size,
        drop_last = config.dataloader.drop_last,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    # model
    model = Model(config.model)
    logging.info(f'Total params:     {sum([ p.numel() for p in model.parameters()                    ])/float(1e6):.2f}M')
    logging.info(f'Trainable params: {sum([ p.numel() for p in model.parameters() if p.requires_grad ])/float(1e6):.2f}M')
    model = model.to(G_DEVICE)

    # optimizer
    optimizer = torch.optim.SGD(model.parameters(), lr = config.training.optimizer.SGD.lr)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.training.scheduler.gamma)

    num_epochs = config.training.num_epochs
    for e in range(1, 1 + num_epochs): # epoch index starts from 1
        # training
        logging.info(f'Training epoch {e} with LR={scheduler.get_last_lr()} ...')
        total_loss = 0.0
        total_utts, total_frames = 0, 0
        num_batches = len(train_dataloader)
        for b, batch in enumerate(train_dataloader, 1): # batch index starts from 1
            samples, X, L, T, U = batch
            num_utts = len(samples)
            num_frames = int(T.sum())

            X, L, T, U = move_tensor_to_device(X, L, T, U)
            Y = model(X)
            loss = Loss(Y, L, T, U)

            utt_loss = loss / num_utts
            frame_loss = loss / num_frames

            utt_loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            total_loss += loss.item()
            total_utts += num_utts
            total_frames += num_frames
            if b % config.training.log_interval == 0:
                logging.info(f'  [{e}/{num_epochs}]:[{b}/{num_batches}] loss={total_loss / total_frames:.2f}')
        scheduler.step()
        logging.info(f'TRAIN[{e}] average_frame_loss={(total_loss / total_frames):.2f} over {total_frames} frames')

        checkpoint_dir = os.path.join('checkpoints')
        os.makedirs(os.path.join(checkpoint_dir), exist_ok = True)
        checkpoint = os.path.join(checkpoint_dir, f'{e}.pt')
        torch.save(model.state_dict(), checkpoint)

        # validation
        logging.info(f'Validating epoch {e} ...')
        total_loss = 0.0
        total_utts, total_frames = 0, 0
        with torch.no_grad():
            for b, batch in enumerate(valid_dataloader):
                samples, X, L, T, U = batch
                num_utts = len(samples)
                num_frames = int(T.sum())

                X, L, T, U = move_tensor_to_device(X, L, T, U)
                Y = model(X)
                loss = Loss(Y, L, T, U)

                best_paths, b = ctc_greedy_search(Y, T)
                print(best_paths, b)
                for p in best_paths:
                    print(tokenizer.sentence_piece.DecodeIds(p))
                
                total_loss += loss.item()
                total_utts += num_utts
                total_frames += num_frames
        logging.info(f'VALID[{e}] average_frame_loss={(total_loss / total_frames):.2f}')

        # with torch.no_grad():
        #     test_model = Model(config.model)
        #     test_model.load_state_dict(torch.load(checkpoint))


def unit_test():
    def test_length_to_mask():
        length=torch.Tensor([1,2,3])
        mask=make_length_mask(length)
        expected = torch.Tensor(
            [
                [1, 0, 0],
                [1, 1, 0],
                [1, 1, 1]
            ]
        ).bool()
        assert torch.equal(mask, expected)

    test_length_to_mask()


if __name__ == '__main__':
    run()
    #unit_test()
