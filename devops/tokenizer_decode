#!/usr/bin/env python3
import sys, os
import argparse
from contextlib import contextmanager

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--backend', choices = ['sentencepiece'], default = 'sentencepiece')
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--input", type=str, default='-')
    parser.add_argument("--input_format", choices=["piece", "id"], default="piece")
    parser.add_argument("--output", type=str, default='-')
    args = parser.parse_args()

    @contextmanager
    def open_istream(fname):
        if fname == '-':
            yield sys.stdin
        else:
            yield open(fname, 'r', encoding='utf8')

    @contextmanager
    def open_ostream(fname):
        if fname == '-':
            yield sys.stdout
        else:
            yield open(fname, 'w+', encoding='utf8')

    if args.backend == 'sentencepiece':
        import sentencepiece as spm
        # https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb

        sp = spm.SentencePieceProcessor()
        sp.Load(args.model + '.model')
        
        with open_istream(args.input) as istream, open_ostream(args.output) as ostream:
            for l in istream:
                if args.input_format == "piece":
                    tokens = l.strip().split()
                    decoded = ''.join(sp.DecodePieces(tokens))     
                elif args.input_format == "id":
                    tokens = [ int(x) for x in l.strip().split() ]
                    decoded = ''.join(sp.DecodeIds(tokens))
                else:
                    raise NotImplementedError
            
                print(decoded, file = ostream)

    else:
        raise NotImplementedError(F'Unsupported tokenizer backend: {args.backend}')