#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.

import sys, os, argparse
import sentencepiece as spm
from contextlib import contextmanager

@contextmanager
def open_stream(fname, mode):
    if fname == '-':
        if mode == 'r':
            yield sys.stdin
        elif mode == 'w+':
            yield sys.stdout
        else:
            raise NotImplementedError
    else:
        yield open(fname, mode, encoding='utf8')


def tokenize(model, input_file, output_file, output_format):
    with open_stream(input_file, 'r') as istream, open_stream(output_file, 'w+') as ostream:
        for l in istream:
            if output_format == 'piece':
                pieces = model.EncodeAsPieces(l.strip())
                encoded = ' '.join(pieces)
            elif output_format == 'id':
                ids = model.EncodeAsIds(l.strip())
                encoded = ' '.join([ str(x) for x in ids ])
            else:
                raise NotImplementedError

            print(encoded, file = ostream)


if __name__ == '__main__':
    DESCRIPTION = '''
    e.g:
        cat i.txt | ops/tokenize -m tokenizer.model > o.txt
        ops/tokenize -m tokenizer.model -i i.txt -o o.txt
    '''
    parser = argparse.ArgumentParser(description = DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-n', '--nj', type = int, default = 1)
    parser.add_argument('-m', '--model', type = str, required = True)
    parser.add_argument('-i', '--input', type = str, default = '-')
    parser.add_argument('-o', '--output', type = str, default = '-')
    parser.add_argument('-f', '--format', choices=['piece', 'id'], default = 'piece')
    args = parser.parse_args()
    print(args, file=sys.stderr)

    if args.nj == 1:

        model = spm.SentencePieceProcessor()
        model.load(args.model)
        tokenize(model, args.input, args.output, args.format)

    else:

        import glob

        assert(args.input != '-')
        assert(args.output != '-')

        wdir = args.output + '.dir'
        os.makedirs(wdir, exist_ok = True)

        print(f'Partitioning input {args.input} into {wdir} ...', file = sys.stderr)
        os.system(f'split -n l/{args.nj} {args.input} {os.path.join(wdir, "part.")}')

        print('Tokenizing all parts ...', file = sys.stderr)
        with open(os.path.join(wdir, 'cmds.sh'), 'w+') as f:
            for x in glob.glob(os.path.join(wdir, 'part.*')):
                out = os.path.join(wdir, os.path.basename(x).replace('part', 'out'))
                log = os.path.join(wdir, os.path.basename(x).replace('part', 'log'))
                print(f'ops/tokenize -m {args.model} -i {x} -o {out} -f {args.format} >& {log}', file = f)
        os.system(f'ops/run_para --nj {args.nj} {os.path.join(wdir, "cmds.sh")}')

        print('Merging results ...', file = sys.stderr)
        os.system(f'cat {os.path.join(wdir, "out.*")} > {args.output}')

