#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.
#

import os, sys
import argparse
import csv
from typing import Optional, Any
from dataclasses import dataclass
import logging.config
import math

from omegaconf import OmegaConf
import yaml
import sentencepiece as spm

import numpy
import torch
import torch.nn as nn
import torchaudio#; torchaudio.set_audio_backend('sox_io')


# Global Constants
G_INT16MAX = 32768
G_EPSILON = 1e-10
G_LOG_EPSILON = math.log(G_EPSILON)
G_FEATURE_PADDING_VALUE = float(0.0)
G_LABEL_PADDING_VALUE = int(-1)

G_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

@dataclass
class Sample:
    id: str = ''
    audio: str = ''
    begin: float = 0.0
    duration: float = 0.0
    text: str = ''
    speaker: Any = None
    audio_info: Any = None  # a place to hold entire-audio info such as codec, bit-width, sample_rate, num_channels etc.


class SampleLoader:
    ''' callable wrapper classï¼š
        Load one dataset sample from a dict with one-utterance info,
        input dict is typically yielded by .tsv .jsonl .yaml file reader.

        return None if an utterance doesn't satisfy certain constraints, e.g. duration too short or too long
    '''
    def __init__(self, config):
        self.config = config
    
    def __call__(self, utt: dict, dataset_root: str = '') -> Optional[Sample] :
        sample = Sample()
        for f, attr in self.config.field_map.items():
            assert hasattr(sample, f), F'field:{f} <- attr:{attr} mapping failed, no such field'
            v = utt.get(attr, None)
            if v:
                if f in ['duration', 'begin', ]:
                    v = float(v)
                elif f == 'audio':
                    v = os.path.join(dataset_root, v)
                else:
                    v = v
                setattr(sample, f, v)
        
        # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-i-o
        # torchaudio.info(sample.audio)
        #   -> AudioMetaData(sample_rate=16000, num_frames=31872, num_channels=1, bits_per_sample=16, encoding=PCM_S)
        sample.audio_info = torchaudio.info(sample.audio)
        
        if sample.duration == 0.0:
            entire_audio_duration = float(sample.audio_info.num_frames) / float(sample.audio_info.sample_rate)
            sample.duration = entire_audio_duration

        if float(sample.duration) < self.config.min_duration:
            return None
        if float(sample.duration) > self.config.max_duration:
            return None

        return sample


class Dataset:
    def __init__(self, config_dataset, sample_loader: callable) :
        self.samples = []

        for subset in config_dataset.subsets:
            if subset.max_num_samples <= 0:
                continue
            logging.info(F'Loading dataset: {subset.id} <- ({subset.root}, {subset.metadata})')

            num_subset_samples = 0
            with open(subset.metadata, 'r', encoding='utf8') as f:
                if str.endswith(subset.metadata, '.tsv'):
                    utterance_reader = csv.DictReader(f, delimiter='\t')
                else:
                    raise NotImplementedError

                for utt in utterance_reader:
                    sample = sample_loader(utt, dataset_root = subset.root)
                    if sample != None:
                        self.samples.append(sample)
                        num_subset_samples += 1
                    if num_subset_samples >= subset.max_num_samples:
                        break # early stop of current subset loading

            logging.info(F'Loaded {num_subset_samples} samples from {subset.id}')
        
        # length sort

        # shuffle

                    
    
    def __getitem__(self, index: int):
        return self.samples[index]
    
    def __len__(self):
        return len(self.samples)


def ReadAndResampleAudio(sample: Sample, target_sample_rate: int) :
    waveform, _ = torchaudio.load(
        sample.audio,
        frame_offset = int(sample.audio_info.sample_rate * sample.begin),
        num_frames   = int(sample.audio_info.sample_rate * sample.duration),
        channels_first = True,
    )

    if sample.audio_info.sample_rate != target_sample_rate:
        waveform = torchaudio.transforms.Resample(sample.audio_info.sample_rate, target_sample_rate)(waveform)
    
    return waveform, target_sample_rate


def ExtractFeature(waveform: torch.Tensor, sample_rate: int, config):
    '''
    kaldi:
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/mel-computations.h#L60
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/feature-fbank.h#L62

    torchaudio:
        https://pytorch.org/audio/stable/compliance.kaldi.html
    '''
    return torchaudio.compliance.kaldi.fbank(
        waveform,
        sample_frequency = sample_rate,
        num_mel_bins = config.num_mel_bins,
        dither = config.dither,
        energy_floor = G_EPSILON, # Kaldi default = 0.0; torchaudio default = 1.0; doesn't matter if use_energy = False (default in both)
    )


class Tokenizer:
    def __init__(self, config):
        self.tokenizer = spm.SentencePieceProcessor()
        self.tokenizer.load(config.model)
    
    def __call__(self, text) -> tuple[list[str], list[int]] :
        token_pieces = self.tokenizer.EncodeAsPieces(text)
        token_ids = self.tokenizer.EncodeAsIds(text)
        return token_pieces, token_ids


class DataPipeline:
    def __init__(self, config, tokenizer: callable, text_normalizer: callable = None):
        self.config = config
        self.text_normalizer = text_normalizer
        self.tokenizer = tokenizer

    def __call__(self, samples: list[Sample]):
        features = []
        labels = []
        minibatch_bookkeeper = []
        for sample in samples:
            # raw audio loading and resampling
            waveform, sample_rate = ReadAndResampleAudio(sample, target_sample_rate = self.config.model_sample_rate)

            # speed perturbation

            # volumn perturbation

            # reverb

            # add noise
            
            # feature extraction
            feature = ExtractFeature(waveform, sample_rate, self.config.feature).detach().numpy()

            # specaug

            text = sample.text
            # text normalization
            if self.text_normalizer:
                text = self.text_normalizer(text)

            # tokenization
            token_pieces, token_ids = self.tokenizer(text)

            features.append(feature)
            labels.append(token_ids)
            
            # store everything here for debug purpose
            minibatch_bookkeeper.append(
                (sample.id, waveform, sample_rate, feature, text, token_pieces, token_ids)
            )
        
        # sequence padding & minibatch packing
        # X of shape [batch, max(time), frequency]
        X = torch.nn.utils.rnn.pad_sequence([ torch.from_numpy(x).float() for x in features ], batch_first = True, padding_value = G_FEATURE_PADDING_VALUE)
        X_lens = torch.tensor([ len(x) for x in features ])

        # Y of shape [batch, max(label_length)]
        L = torch.nn.utils.rnn.pad_sequence([ torch.tensor(l) for l in labels ], batch_first = True, padding_value = G_LABEL_PADDING_VALUE)
        L_lens = torch.tensor([ len(l) for l in labels ])

        return X.to(G_DEVICE), L.to(G_DEVICE), X_lens.to(G_DEVICE), L_lens.to(G_DEVICE), minibatch_bookkeeper


class Model(torch.nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.nnet = nn.Sequential(
            nn.Linear(config.idim, config.hidden_layer_dim),
            nn.ReLU(),
#            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
#            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.odim),
            nn.LogSoftmax(dim = 2),
        )
    
    def forward(self, x):
        y = self.nnet(x)
        return y


def Loss(log_probs, targets, input_lens, target_lens, blank_index = 0, reduction="sum"):
    log_probs = torch.einsum("btv->tbv", log_probs)
    loss = torch.nn.functional.ctc_loss(
        log_probs,
        targets,
        input_lens,
        target_lens,
        blank_index,
        zero_infinity=True,
        reduction=reduction,
    )
    return loss


def run():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type = str, required = True)
    args = parser.parse_args()

    config = OmegaConf.load(args.config)
    print(OmegaConf.to_yaml(config), file = sys.stderr, flush = True)

    # setup logging
    if os.path.isfile(config.logging):
        with open(config.logging, 'r', encoding='utf8') as f:
            logging.config.dictConfig(yaml.safe_load(f))
    else:
        logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    # data sets
    sample_loader = SampleLoader(config.sample_loader)
    train_dataset = Dataset(config.dataset.train, sample_loader = sample_loader)
    valid_dataset = Dataset(config.dataset.valid, sample_loader = sample_loader)

    # data pipline
    tokenizer = Tokenizer(config.tokenizer)
    data_pipeline = DataPipeline(config, tokenizer = tokenizer)

    # data loaders
    train_dl = torch.utils.data.DataLoader(
        train_dataset, 
        shuffle = config.dataloader.shuffle,
        batch_size = config.dataloader.batch_size, 
        drop_last = config.dataloader.drop_last,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    valid_dl = torch.utils.data.DataLoader(
        valid_dataset, 
        shuffle = False,
        batch_size = config.dataloader.batch_size, 
        drop_last = False,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    # model
    model = Model(config.model).to(G_DEVICE)
    logging.info(F'Total params:     {sum([ p.numel() for p in model.parameters()                    ])/float(1e6):.2f}M')
    logging.info(F'Trainable params: {sum([ p.numel() for p in model.parameters() if p.requires_grad ])/float(1e6):.2f}M')

    # optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr = config.training.optimizer.Adam.lr)

    for e in range(config.training.num_epochs):
        logging.info(F'Epoch={e}')
        num_utts = 0
        total_loss = 0.0
        for b, minibatch in enumerate(train_dl):
            X, L, X_lens, L_lens, _ = minibatch
            batch_size = X.shape[0]
            loss = Loss(model(X), L, X_lens, L_lens)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_loss += loss
            num_utts += batch_size
            logging.info(F'{e}.{b} loss={(loss / batch_size):.2f}')
        logging.info(F'TRAIN[{e}] loss={(total_loss/num_utts):.2f}')
        
        
        with torch.no_grad():
            total_loss = 0.0
            num_utts = 0
            for b, minibatch in enumerate(valid_dl):
                X, L, X_lens, L_lens, _ = minibatch
                batch_size = X.shape[0]
                loss = Loss(model(X), L, X_lens, L_lens)
                total_loss += loss.item()
                num_utts += batch_size
            logging.info(F'VALID[{e}] loss={(total_loss / num_utts):.2f}')


def test():
    pass


if __name__ == '__main__':
    run()
    #test()
