#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.
#

import os, sys
import argparse
import csv
from copy import deepcopy

import math
import torch
import torchaudio
#torchaudio.set_audio_backend('sox_io')

from omegaconf import OmegaConf

# Global Constants
G_INT16MAX = 32768
G_EPSILON = 1e-10
G_LOG_EPSILON = math.log(G_EPSILON)

class SpeechRecognitionDataset:
    def __init__(
            self,
            dataset_tsv_list: list[str],
            pick_fields: list[str],
            min_utt_duration: float = 0.5,
            max_utt_duration: float = 20.0,
        ) :

        self.samples = []

        for tsv in dataset_tsv_list:
            with open(tsv, 'r', encoding='utf8') as f:
                tsv_reader = csv.DictReader(f, delimiter='\t')
                for item in tsv_reader:
                    # sample filter
                    if float(item['DURATION']) <= min_utt_duration:
                        continue
                    if float(item['DURATION']) >= max_utt_duration:
                        continue

                    # only extract selected fields
                    sample = {}
                    for x in pick_fields:
                        if x == 'DURATION':
                            sample[x] = float(item[x])
                        else:
                            sample[x] = item[x]

                    self.samples.append(sample)
    
    def __getitem__(self, index: int):
        return self.samples[index]
    
    def __len__(self):
        return len(self.samples)


def ReadAndResampleAudio(audio_sample: dict, target_sample_rate: int) :
    '''
    sample:
    {
        'AUDIO': '/path/to/audio.xxx'
        'BEGIN': 0.5,
        'DURATION': 1.2,
    }
    `BEGIN` is optional, with 0.0 as default value
    `DURATION` is optional, with the duration of entire audio file as default value

    NOTE: the usage of multi-channel hasn't been tested yet.

    '''
    audio_path = audio_sample['AUDIO']
    # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-i-o
    # torchaudio.info():
    #   -> AudioMetaData(sample_rate=16000, num_frames=31872, num_channels=1, bits_per_sample=16, encoding=PCM_S)
    info = torchaudio.info(audio_path) 
    sample_rate = info.sample_rate
    entire_audio_duration = float(info.num_frames) / float(info.sample_rate)

    begin = audio_sample.get("BEGIN", 0.0)
    duration = audio_sample.get("DURATION", entire_audio_duration)

    waveform, _ = torchaudio.load(
        audio_path,
        frame_offset = int(begin * sample_rate),
        num_frames = int(duration * sample_rate),
        channels_first = True,
    )

    if sample_rate != target_sample_rate:
        # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#resampling
        waveform = torchaudio.transforms.Resample(sample_rate, target_sample_rate)(waveform)
    
    return waveform, target_sample_rate


def ExtractFeature(waveform: torch.Tensor, sample_rate: int, feature_config):
    '''
    kaldi default: energy_floor=0.0 
        (https://github.com/kaldi-asr/kaldi/blob/master/src/feat/feature-fbank.h#L56)

    torchaudio default: energy_floor=1.0 
        (https://pytorch.org/audio/stable/compliance.kaldi.html)

    `energy_floor` should only affect results when use_energy = `True`,
    where in both toolkits, use_energy are switched to `False` by default
    for safety we just set it to EPSILON
    '''
    return torchaudio.compliance.kaldi.fbank(
        waveform,
        sample_frequency = sample_rate,
        num_mel_bins = feature_config.num_mel_bins,
        dither = feature_config.dither,
        energy_floor = G_EPSILON,
    )

class DataProcessingPipeline:
    def __init__(self, config):
        self.processors = []
        self.config = config

    def __call__(self, minibatch):
        new_minibatch = []
        for sample in minibatch:
            # create new deep-copy so batched samples will be garbage-collected after minibatch training
            s = deepcopy(sample)

            # raw audio loading and resampling
            s['WAVE'], s['SAMPLE_RATE'] = ReadAndResampleAudio(s, self.config.system_sample_rate)
            
            # feature extraction
            s['FEAT'] = ExtractFeature(s['WAVE'], s['SAMPLE_RATE'], self.config.feature)

            new_minibatch.append(s)

        return new_minibatch

def train():
    parser = argparse.ArgumentParser()

    parser.add_argument('--train_data', nargs='+')
    parser.add_argument('--valid_data', nargs='*')
    parser.add_argument('--test_data', nargs='*')
    parser.add_argument('--config', type = str)

    args = parser.parse_args()
    print(args, file=sys.stderr)

    train_list, valid_list, test_list = args.train_data, args.valid_data, args.test_data
    config = OmegaConf.load(args.config)

    train_dataset = SpeechRecognitionDataset(
        train_list,
        pick_fields = ['ID', 'AUDIO', 'DURATION', 'TEXT'],
        min_utt_duration = config.dataset.min_utt_duration,
        max_utt_duration = config.dataset.max_utt_duration,
    )

    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, 
        shuffle = config.dataloader.shuffle,
        batch_size = config.dataloader.batch_size, 
        drop_last = config.dataloader.drop_last,
        collate_fn = DataProcessingPipeline(config),
        num_workers = 4,
    )

    for e in range(config.train.num_epochs):
        for b, batch in enumerate(train_dataloader):
            print(F'<{e}.{b}>')
            print(batch)
            print([len(s['WAVE'][0])/config.system_sample_rate for s in batch])
            print()

def test():
    pass

if __name__ == '__main__':
    train()
    #test()
