#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.
#

import os, sys
import argparse
import csv
import copy
from typing import Optional, Any
from dataclasses import dataclass

import math
import torch
import torchaudio#; torchaudio.set_audio_backend('sox_io')

import sentencepiece as spm
from omegaconf import OmegaConf

# Global Constants
G_INT16MAX = 32768
G_EPSILON = 1e-10
G_LOG_EPSILON = math.log(G_EPSILON)

@dataclass
class Sample:
    id: str = ''
    audio: str = ''
    begin: float = 0.0
    duration: float = 0.0
    text: str = ''
    speaker: Any = None
    # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-i-o
    # torchaudio.info(sample.audio)
    #   -> AudioMetaData(sample_rate=16000, num_frames=31872, num_channels=1, bits_per_sample=16, encoding=PCM_S)
    audio_info: Any = None                    


class SampleLoader:
    ''' SampleLoader is a callable to load one sample from one utterance_info object,
        where `utterance_info` is a small dict from .tsv .jsonl .yaml reader.
        return None if a sample doesn't satisfy certain constraints, e.g. duration
    '''
    def __init__(self, sample_loader_config):
        self.config = sample_loader_config
    
    def __call__(self, utterance_info: dict) -> Optional[Sample] :
        sample = Sample()
        for f, c in self.config.field_map.items():
            assert hasattr(sample, f), F'field:{f} <- column:{c} mapping failed, no such field'
            v = utterance_info.get(c, None)
            if v:
                if c in ['DURATION', 'BEGIN', ]:
                    v = float(v)
                else:
                    v = v
                setattr(sample, f, v)
        
        sample.audio_info = torchaudio.info(sample.audio)
        
        if sample.duration == 0.0:
            entire_audio_duration = float(sample.audio_info.num_frames) / float(sample.audio_info.sample_rate)
            sample.duration = entire_audio_duration

        if float(sample.duration) < self.config.min_duration:
            return None
        if float(sample.duration) > self.config.max_duration:
            return None

        return sample


class SpeechRecognitionDataset:
    def __init__(self, datasets: list[str], sample_loader: callable) :
        self.samples = []

        for dataset in datasets:
            with open(dataset, 'r', encoding='utf8') as f:
                items = csv.DictReader(f, delimiter='\t')
                for item in items:
                    sample = sample_loader(item)
                    if sample != None:
                        self.samples.append(sample)
    
    def __getitem__(self, index: int):
        return self.samples[index]
    
    def __len__(self):
        return len(self.samples)


def ReadAndResampleAudio(sample: Sample, target_sample_rate: int) :
    waveform, _ = torchaudio.load(
        sample.audio,
        frame_offset = int(sample.audio_info.sample_rate * sample.begin),
        num_frames   = int(sample.audio_info.sample_rate * sample.duration),
        channels_first = True,
    )

    if sample.audio_info.sample_rate != target_sample_rate:
        # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#resampling
        waveform = torchaudio.transforms.Resample(sample.audio_info.sample_rate, target_sample_rate)(waveform)
    
    return waveform, target_sample_rate


def ExtractFeature(waveform: torch.Tensor, sample_rate: int, config):
    '''
    kaldi default: energy_floor=0.0 
        (https://github.com/kaldi-asr/kaldi/blob/master/src/feat/feature-fbank.h#L56)

    torchaudio default: energy_floor=1.0 
        (https://pytorch.org/audio/stable/compliance.kaldi.html)

    `energy_floor` should only affect results when use_energy = `True`,
    where in both toolkits, use_energy are switched to `False` by default
    for safety we just set it to EPSILON
    '''
    return torchaudio.compliance.kaldi.fbank(
        waveform,
        sample_frequency = sample_rate,
        num_mel_bins = config.feature.num_mel_bins,
        dither = config.feature.dither,
        energy_floor = G_EPSILON,
    )


class DataProcessingPipeline:
    def __init__(self, config, tokenizer = None, text_normalizer = None):
        self.config = config
        self.text_normalizer = text_normalizer
        self.tokenizer = tokenizer

    def __call__(self, minibatch_samples):
        minibatch = []
        for sample in minibatch_samples:
            # raw audio loading and resampling
            waveform, sample_rate = ReadAndResampleAudio(sample, target_sample_rate = self.config.system_sample_rate)

            # speed perturbation

            # volumn perturbation

            # reverb

            # add noise
            
            # feature extraction
            feature = ExtractFeature(waveform, sample_rate, self.config)

            # specaug

            text = sample.text
            # text normalization
            if self.text_normalizer:
                text = self.text_normalizer(text)

            # tokenization
            token_pieces = self.tokenizer.EncodeAsPieces(text)
            token_ids = self.tokenizer.EncodeAsIds(text)

            minibatch.append(
                (sample.id, waveform, sample_rate, feature, text, token_pieces, token_ids)
            )

        return minibatch


def train():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type = str, required = True)
    args = parser.parse_args()

    config = OmegaConf.load(args.config)
    print(config, file = sys.stderr)

    # dataloaders
    sample_loader = SampleLoader(config.sample_loader)
    train_dataset = SpeechRecognitionDataset(config.train_list, sample_loader)

    # tokenizer
    tokenizer = spm.SentencePieceProcessor()
    tokenizer.load(config.tokenizer.model)

    # datasets
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, 
        shuffle = config.dataloader.shuffle,
        batch_size = config.dataloader.batch_size, 
        drop_last = config.dataloader.drop_last,
        collate_fn = DataProcessingPipeline(config, tokenizer = tokenizer),
        #num_workers = 4,
    )

    for e in range(config.train.num_epochs):
        for b, batch in enumerate(train_dataloader):
            print(F'----- {e}.{b} -----')
            print(batch)

def test():
    pass


if __name__ == '__main__':
    train()
    #test()
