#!/usr/bin/env bash

if [ $# != 4 ]; then
    echo "$0 <text> <tokenizer> <working_dir> <arpa_name>"
    echo "e.g. $0 train.txt model_dir/tokenizer wdir 4gram"
    exit 1
fi

text=$1
tokenizer=$2
dir=$3
arpa_name=$4

mkdir -p $dir

echo "Generating KenLM vocabulary..."
vocab=${tokenizer}.vocab
cat $vocab \
    | awk '{print $1}' \
    | grep -v '<blk>' \
    | grep -v '<s>' | grep -v '</s>' \
    | grep -v '<unk>' | grep -v '<UNK>' \
    > $dir/${arpa_name}.vocab

echo "Training arpa ..."
if ! which lmplz >& /dev/null ; then
    echo "$0: cannot find training tool *lmplz*."
    exit 1
fi

train_opts="-o 6"  # e.g. "-o 4 -S 50% --prune 0 5 7 7"
cat $dir/${arpa_name}.vocab $text \
    | lmplz $train_opts --limit_vocab_file $dir/${arpa_name}.vocab \
    > $dir/${arpa_name}.arpa


echo "Building KenLM binary from arpa..."
if ! which build_binary >& /dev/null ; then
    echo "$0: cannot find KenLM's build_binary tool,"
    echo "check kenlm installation (tools/extras/install_kenlm_query_only.sh)."
    exit 1
fi

build_opts="" # e.g. "-q 8 -b 8" for 8bits quantization
model_type="trie" # "trie" or "probing". trie is smaller, probing is faster.
build_binary  $build_opts  $model_type  $dir/${arpa_name}.arpa  $dir/${arpa_name}.${model_type}

