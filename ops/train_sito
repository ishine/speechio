#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.
#

import os, sys
import argparse
import csv
from typing import Optional, Any
from dataclasses import dataclass
import logging.config
import math

from omegaconf import OmegaConf
import yaml
import sentencepiece as spm

import numpy
import torch
import torch.nn as nn
import torchaudio

import pandas as pd
#import k2

# Global Constants
G_INT16MAX = 32768
G_EPSILON = torch.finfo(torch.float).eps # numeric_limits<float>::epsilon() 1.1920928955078125e-07
G_LOG_EPSILON = math.log(G_EPSILON)
G_FEATURE_PADDING_VALUE = float(0.0)
G_LABEL_PADDING_VALUE = int(-1)
G_16BIT_PCM_SAMPLE_MAX = 1 << 15
G_BLANK_ID = 0
G_BOS_ID = 1
G_EOS_ID = 2

G_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

@dataclass
class Sample:
    id: str = ''
    audio: str = ''
    begin: float = 0.0
    duration: float = 0.0
    text: str = ''
    speaker: str = ''


class SampleLoader:
    ''' A callable wrapper class to load one dataset sample from one utterance
    input:
        utt dict is typically yielded by .tsv .jsonl .yaml file reader.

    return:
        a Sample object or None
        None if an utterance doesn't satisfy constraints e.g. duration, text length
    '''
    def __init__(self, config, data_root: str = ''):
        self.config = config
        self.data_root = data_root
    
    def __call__(self, utt: dict) -> Optional[Sample] :
        sample = Sample()
        for attr, field in self.config.field_map.items():
            if v := utt.get(field):
                if attr in ['duration', 'begin', ]:
                    v = float(v)
                elif attr == 'audio':
                    v = os.path.join(self.data_root, v)
                else:
                    v = str(v)
                assert hasattr(sample, attr), F'{field} -> Sample.{attr} mapping failed, no such attribute'
                setattr(sample, attr, v)

        if sample.duration < self.config.min_duration:
            return None
        if sample.duration > self.config.max_duration:
            return None
        if len(sample.text) < self.config.min_text_length:
            return None
        if len(sample.text) > self.config.max_text_length:
            return None

        return sample


class Dataset:
    def __init__(self, config_data_zoo, config_dataset, config_sample_loader) :
        self.samples = []
        data_zoo = OmegaConf.load(config_data_zoo)
        for subset in config_dataset.subsets:
            if subset.max_num_samples == 0:
                continue
            elif subset.max_num_samples < 0:
                subset.max_num_samples = sys.maxsize # https://docs.python.org/3/library/sys.html#sys.maxsize

            subset_root, subset_meta = data_zoo[subset.id].root, data_zoo[subset.id].metadata
            logging.info(F'Loading dataset: {subset.id} from ({subset_root} : {subset_meta})')
            with open(subset_meta, 'r', encoding='utf8') as f:
                if str.endswith(subset_meta, '.tsv'):
                    utterance_reader = csv.DictReader(f, delimiter='\t')
                else:
                    raise NotImplementedError

                k = 0
                sample_loader = SampleLoader(config = config_sample_loader, data_root = subset_root)
                for utt in utterance_reader:
                    if k < subset.max_num_samples:
                        if sample := sample_loader(utt):
                            self.samples.append(sample)
                            k += 1
                logging.info(F'{k} samples loaded from {subset.id}')

        # length sort

        # shuffle

                    
    def __getitem__(self, index: int):
        return self.samples[index]
    
    def __len__(self):
        return len(self.samples)


def ReadAndResampleAudio(sample: Sample, target_sample_rate: int) :
    '''
    https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-i-o

    torchaudio.info(sample.audio)
    -> 
    AudioMetaData(
        sample_rate=16000,
        num_frames=31872,
        num_channels=1,
        bits_per_sample=16,
        encoding=PCM_S,
    )
    '''
    audio_info = torchaudio.info(sample.audio)

    waveform, _ = torchaudio.load(
        sample.audio,
        frame_offset = int(audio_info.sample_rate * sample.begin),
        num_frames   = int(audio_info.sample_rate * sample.duration),
        channels_first = True,
    )

    if audio_info.sample_rate != target_sample_rate:
        waveform = torchaudio.transforms.Resample(audio_info.sample_rate, target_sample_rate)(waveform)
    
    return waveform, target_sample_rate


def ExtractFeature(waveform: torch.Tensor, sample_rate: int, config):
    '''
    kaldi:
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/mel-computations.h#L60
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/feature-fbank.h#L62

    torchaudio:
        https://pytorch.org/audio/stable/compliance.kaldi.html
    '''
    # By default, torchaudio uses normalized 32bits float waveform [-1.0, 1.0]
    assert waveform.dtype == torch.float32

    # To use Kaldi compliance function, waveform needs to be convert to 16bits signed-interger PCM [-32768, 32767]
    # TODO:
    # torchaudio's feature extraction is not consistent with Kaldi https://github.com/pytorch/audio/issues/400
    # torchaudio official plan to bind kaldi feat lib: https://github.com/pytorch/audio/issues/1269
    # to not finished up to 2021-10-02 : https://github.com/pytorch/audio/pull/1326
    # keep an eye on this work and do it myself if necessary
    waveform = (waveform * G_16BIT_PCM_SAMPLE_MAX)

    feature = torchaudio.compliance.kaldi.fbank(
        waveform,
        sample_frequency = sample_rate,
        num_mel_bins = config.num_mel_bins,
        dither = config.dither,
        energy_floor = G_EPSILON,
        # about energy_floor:
        #   Kaldi default = 0.0; torchaudio default = 1.0;
        #   it doesn't matter if use_energy = False (default in both)
    )
    return feature


class Tokenizer:
    def __init__(self, config):
        self.tokenizer = spm.SentencePieceProcessor()
        self.tokenizer.load(config.model)
        self.vocab = [ [ self.tokenizer.id_to_piece(x), x] for x in range(self.tokenizer.get_piece_size())]
    
    def __call__(self, text) -> tuple[list[str], list[int]] :
        token_pieces = self.tokenizer.EncodeAsPieces(text)
        token_ids = self.tokenizer.EncodeAsIds(text)
        return token_pieces, token_ids


class DataPipeline:
    def __init__(self, config, tokenizer: Optional[callable] = None, text_normalizer: Optional[callable] = None, mean_norm_shift: Optional[torch.Tensor] = None, var_norm_scale: Optional[torch.Tensor] = None):
        self.config = config

        self.text_normalizer = text_normalizer
        self.tokenizer = tokenizer

        self.mean_norm_shift = mean_norm_shift
        self.var_norm_scale = var_norm_scale

    def __call__(self, samples: list[Sample]):
        minibatch = []
        for sample in samples:
            # raw audio loading and resampling
            waveform, sample_rate = ReadAndResampleAudio(sample, target_sample_rate = self.config.model_sample_rate)
            # speed perturbation

            # volumn perturbation

            # reverb

            # add noise
            
            # feature extraction
            feature = ExtractFeature(waveform, sample_rate, self.config.feature)
            feature = feature.detach()

            # specaug

            # mean normalization
            if (self.mean_norm_shift != None):
                feature += self.mean_norm_shift

            # variance normalizaton
            if (self.var_norm_scale != None):
                feature *= self.var_norm_scale

            text = sample.text
            # text normalization
            if self.text_normalizer:
                text = self.text_normalizer(text)

            # tokenization
            token_pieces, token_ids = [], []
            if self.tokenizer:
                token_pieces, token_ids = self.tokenizer(text)

            # store everything here for debug purpose
            minibatch.append({
                'id': sample.id,
                'waveform': waveform,
                'sample_rate': sample_rate,
                'feature': feature,
                'text': text,
                'token_pieces': token_pieces,
                'token_ids': token_ids,
            })

        features = [ x['feature'] for x in minibatch ]
        time_lengths = torch.tensor([ len(x) for x in features ]) # [B]
        inputs = nn.utils.rnn.pad_sequence(
            features,
            batch_first = True,
            padding_value = G_FEATURE_PADDING_VALUE,
        ) # [B, max(time_lengths), F]

        labels = [ x['token_ids'] for x in minibatch ]
        label_lengths = torch.tensor([ len(y) for y in labels ]) # [B]   
        targets = nn.utils.rnn.pad_sequence(
            [ torch.tensor(l) for l in labels ],
            batch_first = True,
            padding_value = G_LABEL_PADDING_VALUE,
        ) # [B, max(label_lengths)]

        return minibatch, inputs, targets, time_lengths, label_lengths


class Model(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.input_size = config.idim
        self.hidden_size = config.hidden_layer_dim
        self.num_layers = config.num_hidden_layers
        self.num_classes = config.odim

        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(self.hidden_size*2, self.num_classes)  # 2 for bidirection
        self.log_softmax = nn.LogSoftmax(dim = 2)
    
    def forward(self, x):
        # Set initial states
        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(G_DEVICE) # 2 for bidirection 
        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(G_DEVICE)
        
        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)
        # Decode the hidden state of the last time step
        out = self.fc(out)
        out = self.log_softmax(out)
        return out


def Loss(log_probs, labels, input_lens, label_lens, blank_index = G_BLANK_ID):
    log_probs = torch.einsum("btv->tbv", log_probs)
    loss = torch.nn.functional.ctc_loss(
        log_probs,
        labels,
        input_lens,
        label_lens,
        blank_index,
        zero_infinity=True,
        reduction='sum',
    )
    return loss


def compute_mvn(dataset, config):
    pipeline = DataPipeline(config)

    # data loaders
    dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle = False,
        batch_size = 32, 
        drop_last = False,
        num_workers = 4,
        collate_fn = pipeline,
    )

    logging.info('Computing global mean variance stats')
    M = torch.zeros(config.model.idim)
    V = torch.zeros(config.model.idim)
    k = 0
    for batch in dataloader:
        samples, _,_,_,_ = batch
        for sample in samples:
            x = sample['feature']  # [Time, Freq]
            M += x.sum(dim=0)
            V += x.square().sum(dim=0)
            k += x.shape[0]
    M = M / k
    V = V / k - M.square()
    mean_norm_shift = -M
    var_norm_scale = V.sqrt().clamp(min = G_EPSILON).reciprocal()
    logging.info(F'Global MEAN/VAR Normalizer after {k} frames:\nmean_norm_shift:  {mean_norm_shift}\nvar_norm_scale:  {var_norm_scale}')
    return mean_norm_shift, var_norm_scale


def length_to_mask(lengths: torch.Tensor) -> torch.Tensor:
    """Make mask tensor indicating in-length elements.
    Args:
        lengths (torch.Tensor): Batch of lengths (B,).
    Returns:
        torch.Tensor: Bool tensor mask indicating in-length elements.
    """
    B = int(lengths.size(0))
    L = int(lengths.max().item())
    range_tensor = torch.arange(0, L, dtype=torch.int64, device=lengths.device).unsqueeze(0).expand(B, L)
    mask = range_tensor < lengths.unsqueeze(-1)
    return mask


def ctc_collapse_sequence(hyp: list[int]) -> list[int]:
    new_hyp: list[int] = []
    cur = 0
    while cur < len(hyp):
        if hyp[cur] != G_BLANK_ID:
            new_hyp.append(hyp[cur])
        prev = cur
        while cur < len(hyp) and hyp[cur] == hyp[prev]:
            cur += 1
    return new_hyp


def ctc_greedy_search(log_probs: torch.Tensor, time_lengths: torch.Tensor):
    B = log_probs.shape[0]

    best_scores, best_paths = log_probs.topk(1, dim=2) # [B,T,V] -> [B,T,k=1]

    best_paths = best_paths.squeeze(2)
    best_scores = best_scores.squeeze(2)
    pad_mask = ~ length_to_mask(time_lengths)

    best_paths.masked_fill_(pad_mask, G_EOS_ID)
    best_scores.masked_fill_(pad_mask, 0.0)
    scores = best_scores.sum(1)
    hyps = [ ctc_collapse_sequence(p.tolist()) for p in best_paths ]
    #hyps = [ p.tolist() for p in best_paths ]
    return hyps, scores


def DumpTensorToCSV(x: torch.Tensor, filename: str):
    df = pd.DataFrame(x.numpy())
    df.to_csv(filename)


def ensure_tensor_in_device(*tensors):
    return tuple([t.to(G_DEVICE) for t in tensors])


def run():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type = str, required = True)
    args = parser.parse_args()

    config = OmegaConf.load(args.config)
    print(OmegaConf.to_yaml(config), file = sys.stderr, flush = True)

    # setup logging
    if os.path.isfile(config.logging):
        with open(config.logging, 'r', encoding='utf8') as f:
            logging.config.dictConfig(yaml.safe_load(f))
    else:
        logging.basicConfig(
            stream=sys.stderr, level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s'
        )

    # load train/dev/test dataset from data zoo
    train_dataset = Dataset(config.data_zoo, config.dataset.train, config.sample_loader)
    valid_dataset = Dataset(config.data_zoo, config.dataset.valid, config.sample_loader)

    # compute feature mean & var normalizer
    mean_norm_shift, var_norm_scale = compute_mvn(train_dataset, config)
    #torch.save(mean_norm_shift, 'mean_norm_shift.pt')
    #torch.save(var_norm_scale, 'var_norm_scale.pt')

    # data pipline
    tokenizer = Tokenizer(config.tokenizer)
    data_pipeline = DataPipeline(config, tokenizer = tokenizer, mean_norm_shift = mean_norm_shift, var_norm_scale = var_norm_scale)

    # data loaders
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, 
        shuffle = config.dataloader.shuffle,
        batch_size = config.dataloader.batch_size, 
        drop_last = config.dataloader.drop_last,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    valid_dataloader = torch.utils.data.DataLoader(
        valid_dataset, 
        shuffle = False,
        batch_size = config.dataloader.batch_size, 
        drop_last = False,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    # model
    model = Model(config.model)
    logging.info(F'Total params:     {sum([ p.numel() for p in model.parameters()                    ])/float(1e6):.2f}M')
    logging.info(F'Trainable params: {sum([ p.numel() for p in model.parameters() if p.requires_grad ])/float(1e6):.2f}M')
    model = model.to(G_DEVICE)

    # optimizer
    optimizer = torch.optim.SGD(model.parameters(), lr = config.training.optimizer.SGD.lr)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.training.scheduler.gamma)

    num_epochs = config.training.num_epochs
    for e in range(num_epochs):
        # training
        logging.info(F'Training epoch {e} with LR={scheduler.get_last_lr()} ...')
        total_loss = 0.0
        total_utts, total_frames = 0, 0
        num_batches = len(train_dataloader)
        for b, batch in enumerate(train_dataloader):
            samples, X, L, T, U = batch
            num_utts = len(samples)
            num_frames = int(T.sum())

            X, L, T, U = ensure_tensor_in_device(X, L, T, U)
            Y = model(X)
            loss = Loss(Y, L, T, U)

            utt_loss = loss / num_utts
            frame_loss = loss / num_frames

            utt_loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            total_loss += loss.item()
            total_utts += num_utts
            total_frames += num_frames
            if b % config.training.log_interval == 0:
                logging.info(F'  [{e}/{num_epochs}]:[{b}/{num_batches}] loss={total_loss / total_frames:.2f}')
        scheduler.step()
        logging.info(F'TRAIN[{e}] average_frame_loss={(total_loss / total_frames):.2f}')

        checkpoint_dir = os.path.join('checkpoints')
        os.makedirs(os.path.join(checkpoint_dir), exist_ok = True)
        checkpoint = os.path.join(checkpoint_dir, F'{e}.pt')
        torch.save(model.state_dict(), checkpoint)

        # validation
        logging.info(F'Validating epoch {e} ...')
        total_loss = 0.0
        total_utts, total_frames = 0, 0
        with torch.no_grad():
            for b, batch in enumerate(valid_dataloader):
                samples, X, L, T, U = batch
                num_utts = len(samples)
                num_frames = int(T.sum())

                X, L, T, U = ensure_tensor_in_device(X, L, T, U)
                Y = model(X)
                loss = Loss(Y, L, T, U)

                best_paths, b = ctc_greedy_search(Y, T)
                print(best_paths, b)
                for p in best_paths:
                    print(tokenizer.tokenizer.DecodeIds(p))
                
                total_loss += loss.item()
                total_utts += num_utts
                total_frames += num_frames
        logging.info(F'VALID[{e}] average_frame_loss={(total_loss / total_frames):.2f}')

        # with torch.no_grad():
        #     test_model = Model(config.model)
        #     test_model.load_state_dict(torch.load(checkpoint))


def unit_test():
    def test_length_to_mask():
        length=torch.Tensor([1,2,3])
        mask=length_to_mask(length)
        expected = torch.Tensor(
            [
                [1, 0, 0],
                [1, 1, 0],
                [1, 1, 1]
            ]
        ).bool()
        assert torch.equal(mask, expected)

    test_length_to_mask()


if __name__ == '__main__':
    run()
    #unit_test()
