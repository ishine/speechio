#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.
#

import os, sys
import argparse
import csv
from typing import Optional, Any
from dataclasses import dataclass
import logging.config
import math

from omegaconf import OmegaConf
import yaml
import sentencepiece as spm

import pandas as pd

import numpy
import torch
import torch.nn as nn
import torchaudio

import k2

# Global Constants
G_INT16MAX = 32768
G_EPSILON = torch.finfo(torch.float).eps # numeric_limits<float>::epsilon() 1.1920928955078125e-07
G_LOG_EPSILON = math.log(G_EPSILON)
G_FEATURE_PADDING_VALUE = float(0.0)
G_LABEL_PADDING_VALUE = int(-1)
G_16BIT_PCM_SAMPLE_MAX = 1 << 15
G_BLANK_ID = 0
G_BOS_ID = 1
G_EOS_ID = 2

G_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

@dataclass
class Sample:
    id: str = ''
    audio: str = ''
    begin: float = 0.0
    duration: float = 0.0
    text: str = ''
    speaker: Any = None


class SampleLoader:
    ''' A callable wrapper class to load one dataset sample from one utterance
    input:
        utt dict is typically yielded by .tsv .jsonl .yaml file reader.

    return:
        a Sample object or None
        None if an utterance doesn't satisfy constraints e.g. duration, text length
    '''
    def __init__(self, config):
        self.config = config
    
    def __call__(self, utt: dict, dataset_root: str = '') -> Optional[Sample] :
        sample = Sample()
        for f, attr in self.config.field_map.items():
            assert hasattr(sample, f), F'field:{f} <- attr:{attr}, no such field'
            v = utt.get(attr, None)
            if v:
                if f in ['duration', 'begin', ]:
                    v = float(v)
                elif f == 'audio':
                    v = os.path.join(dataset_root, v)
                else:
                    v = v
                setattr(sample, f, v)

        if float(sample.duration) < self.config.min_duration:
            return None
        if float(sample.duration) > self.config.max_duration:
            return None

        return sample


class Dataset:
    def __init__(self, config_dataset, sample_loader: callable) :
        self.samples = []

        for subset in config_dataset.subsets:
            if subset.max_num_samples == 0:
                continue
            elif subset.max_num_samples < 0:
                subset.max_num_samples = sys.maxsize # https://docs.python.org/3/library/sys.html#sys.maxsize

            logging.info(F'Loading dataset: {subset.id} <- ({subset.root}, {subset.metadata})')
            n = 0
            with open(subset.metadata, 'r', encoding='utf8') as f:
                if str.endswith(subset.metadata, '.tsv'):
                    utterance_reader = csv.DictReader(f, delimiter='\t')
                else:
                    raise NotImplementedError

                for utt in utterance_reader:
                    sample = sample_loader(utt, dataset_root = subset.root)
                    if sample != None:
                        self.samples.append(sample)
                        n += 1
                    if n >= subset.max_num_samples:
                        break # early stop of current subset loading
            logging.info(F'Loaded {n} samples from {subset.id}')
        
        # length sort

        # shuffle

                    
    def __getitem__(self, index: int):
        return self.samples[index]
    
    def __len__(self):
        return len(self.samples)


def ReadAndResampleAudio(sample: Sample, target_sample_rate: int) :
    '''
    https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-i-o

    torchaudio.info(sample.audio)
    -> 
    AudioMetaData(
        sample_rate=16000,
        num_frames=31872,
        num_channels=1,
        bits_per_sample=16,
        encoding=PCM_S,
    )
    '''
    audio_info = torchaudio.info(sample.audio)

    waveform, _ = torchaudio.load(
        sample.audio,
        frame_offset = int(audio_info.sample_rate * sample.begin),
        num_frames   = int(audio_info.sample_rate * sample.duration),
        channels_first = True,
    )

    if audio_info.sample_rate != target_sample_rate:
        waveform = torchaudio.transforms.Resample(audio_info.sample_rate, target_sample_rate)(waveform)
    
    return waveform, target_sample_rate


def ExtractFeature(waveform: torch.Tensor, sample_rate: int, config):
    '''
    kaldi:
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/mel-computations.h#L60
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/feature-fbank.h#L62

    torchaudio:
        https://pytorch.org/audio/stable/compliance.kaldi.html
    '''
    # torchaudio normally uses normalized wave
    assert waveform.dtype == torch.float32
    # To use Kaldi compliance function, 16bit signed-PCM is required
    # TODO:
    # torchaudio's feature extraction is not consistent with Kaldi https://github.com/pytorch/audio/issues/400
    # torchaudio official plan to bind kaldi feat lib: https://github.com/pytorch/audio/issues/1269
    # to not finished up to 2021-10-02 : https://github.com/pytorch/audio/pull/1326
    # keep an eye on this work and do it myself if necessary
    waveform = (waveform * G_16BIT_PCM_SAMPLE_MAX)

    feature = torchaudio.compliance.kaldi.fbank(
        waveform,
        sample_frequency = sample_rate,
        num_mel_bins = config.num_mel_bins,
        dither = config.dither,
        energy_floor = G_EPSILON,
        # about energy_floor:
        #   Kaldi default = 0.0; torchaudio default = 1.0;
        #   it doesn't matter if use_energy = False (default in both)
    )
    return feature


class Tokenizer:
    def __init__(self, config):
        self.tokenizer = spm.SentencePieceProcessor()
        self.tokenizer.load(config.model)
        self.vocab = [ [ self.tokenizer.id_to_piece(x), x] for x in range(self.tokenizer.get_piece_size())]
    
    def __call__(self, text) -> tuple[list[str], list[int]] :
        token_pieces = self.tokenizer.EncodeAsPieces(text)
        token_ids = self.tokenizer.EncodeAsIds(text)
        return token_pieces, token_ids


class DataPipeline:
    def __init__(self, config, tokenizer: Optional[callable] = None, text_normalizer: Optional[callable] = None, mean_norm_shift: Optional[torch.Tensor] = None, var_norm_scale: Optional[torch.Tensor] = None):
        self.config = config

        self.text_normalizer = text_normalizer
        self.tokenizer = tokenizer

        self.mean_norm_shift = mean_norm_shift
        self.var_norm_scale = var_norm_scale

    def __call__(self, samples: list[Sample]):
        features = []
        labels = []
        minibatch = []
        for sample in samples:
            # raw audio loading and resampling
            waveform, sample_rate = ReadAndResampleAudio(sample, target_sample_rate = self.config.model_sample_rate)
            # speed perturbation

            # volumn perturbation

            # reverb

            # add noise
            
            # feature extraction
            feature = ExtractFeature(waveform, sample_rate, self.config.feature)
            feature = feature.detach()

            # specaug

            # mean normalization
            if (self.mean_norm_shift != None):
                feature += self.mean_norm_shift

            # variance normalizaton
            if (self.var_norm_scale != None):
                feature *= self.var_norm_scale

            features.append(feature)

            text = sample.text
            # text normalization
            if self.text_normalizer:
                text = self.text_normalizer(text)

            # tokenization
            token_pieces, token_ids = [], []
            if self.tokenizer:
                token_pieces, token_ids = self.tokenizer(text)

            labels.append(token_ids)
            
            # store everything here for debug purpose
            minibatch.append(
                (sample.id, waveform, sample_rate, feature, text, token_pieces, token_ids)
            )
        
        # sequence padding & minibatch packing
        # X: [B, T, F], X_lens: [B, 1]
        X = torch.nn.utils.rnn.pad_sequence(features, batch_first = True, padding_value = G_FEATURE_PADDING_VALUE)
        X_lens = torch.tensor([ len(x) for x in features ])

        # L: [B, U], L_lens: [B, 1]
        L = torch.nn.utils.rnn.pad_sequence([ torch.tensor(l) for l in labels ], batch_first = True, padding_value = G_LABEL_PADDING_VALUE)
        L_lens = torch.tensor([ len(l) for l in labels ])

        return minibatch, X, L, X_lens, L_lens


class Model(torch.nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.nnet = nn.Sequential(
            nn.Linear(config.idim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.odim),
            nn.LogSoftmax(dim = 2),
        )
    
    def forward(self, x):
        y = self.nnet(x)
        return y


def Loss(log_probs, labels, input_lens, label_lens, blank_index = 0):
    log_probs = torch.einsum("btv->tbv", log_probs)
    loss = torch.nn.functional.ctc_loss(
        log_probs,
        labels,
        input_lens,
        label_lens,
        blank_index,
        zero_infinity=True,
        reduction='sum',
    )
    return loss


def ComputeMvnNormalizer(dataset, config):
    pipeline = DataPipeline(config)

    # data loaders
    dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle = False,
        batch_size = 32, 
        drop_last = False,
        num_workers = 4,
        collate_fn = pipeline,
    )

    logging.info('Computing global mean variance stats')
    M = torch.zeros(config.model.idim)
    V = torch.zeros(config.model.idim)
    k = 0
    for batch in dataloader:
        samples, _,_,_,_ = batch
        for sample in samples:
            feat = sample[3]  # [Time, Freq]
            M += feat.sum(dim=0)
            V += feat.square().sum(dim=0)
            k += feat.shape[0]
    M = M / k
    V = (V / k - M.square())
    mean_norm_shift = -M
    var_norm_scale = V.sqrt().clamp(min = G_EPSILON).reciprocal()
    logging.info(F'Global MEAN/VAR Normalizer after {k} frames:\nmean_norm_shift:  {mean_norm_shift}\nvar_norm_scale:  {var_norm_scale}')
    return mean_norm_shift, var_norm_scale


def make_pad_mask(lengths: torch.Tensor) -> torch.Tensor:
    """Make mask tensor containing indices of padded part.

    See description of make_non_pad_mask.

    Args:
        lengths (torch.Tensor): Batch of lengths (B,).
    Returns:
        torch.Tensor: Mask tensor containing indices of padded part.

    Examples:
        >>> lengths = [5, 3, 2]
        >>> make_pad_mask(lengths)
        masks = [[0, 0, 0, 0 ,0],
                 [0, 0, 0, 1, 1],
                 [0, 0, 1, 1, 1]]
    """
    batch_size = int(lengths.size(0))
    max_len = int(lengths.max().item())
    seq_range = torch.arange(0,
                             max_len,
                             dtype=torch.int64,
                             device=lengths.device)
    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)
    seq_length_expand = lengths.unsqueeze(-1)
    mask = seq_range_expand >= seq_length_expand
    return mask

def make_nonpad_mask(lengths: torch.Tensor) -> torch.Tensor:
    return ~make_pad_mask(lengths)


def collapse_ctc_sequence(hyp: list[int]) -> list[int]:
    new_hyp: list[int] = []
    cur = 0
    while cur < len(hyp):
        if hyp[cur] != G_BLANK_ID:
            new_hyp.append(hyp[cur])
        prev = cur
        while cur < len(hyp) and hyp[cur] == hyp[prev]:
            cur += 1
    return new_hyp


def CtcGreedySearch(log_probs: torch.Tensor, X_lens: torch.Tensor):
    B = log_probs.shape[0]
    T = log_probs.shape[1]

    best_scores, best_paths = log_probs.topk(1, dim=2) # [B,T,V] -> [B,T,k=1]

    best_paths = best_paths.squeeze(2)
    best_scores = best_scores.squeeze(2)
    m = make_pad_mask(X_lens)

    best_paths.masked_fill_(m, G_EOS_ID)
    best_scores.masked_fill_(m, 0.0)
    scores = best_scores.sum(1)
    hyps = [ collapse_ctc_sequence(p.tolist()) for p in best_paths ]
    #hyps = [ p.tolist() for p in best_paths ]
    return hyps, scores

def DumpTensorToCSV(x: torch.Tensor, filename: str):
    df = pd.DataFrame(x.numpy())
    df.to_csv(filename)


def run():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type = str, required = True)
    args = parser.parse_args()

    config = OmegaConf.load(args.config)
    print(OmegaConf.to_yaml(config), file = sys.stderr, flush = True)

    # setup logging
    if os.path.isfile(config.logging):
        with open(config.logging, 'r', encoding='utf8') as f:
            logging.config.dictConfig(yaml.safe_load(f))
    else:
        logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    # data sets
    sample_loader = SampleLoader(config.sample_loader)
    train_dataset = Dataset(config.dataset.train, sample_loader = sample_loader)
    valid_dataset = Dataset(config.dataset.valid, sample_loader = sample_loader)

    # data pipline
    mean_norm_shift, var_norm_scale = ComputeMvnNormalizer(train_dataset, config)
    #torch.save(mean_norm_shift, 'mean_norm_shift.pt')
    #torch.save(var_norm_scale, 'var_norm_scale.pt')

    tokenizer = Tokenizer(config.tokenizer)
    data_pipeline = DataPipeline(config, tokenizer = tokenizer, mean_norm_shift = mean_norm_shift, var_norm_scale = var_norm_scale)

    # data loaders
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, 
        shuffle = config.dataloader.shuffle,
        batch_size = config.dataloader.batch_size, 
        drop_last = config.dataloader.drop_last,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    valid_dataloader = torch.utils.data.DataLoader(
        valid_dataset, 
        shuffle = False,
        batch_size = config.dataloader.batch_size, 
        drop_last = False,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    # model
    model = Model(config.model)
    logging.info(F'Total params:     {sum([ p.numel() for p in model.parameters()                    ])/float(1e6):.2f}M')
    logging.info(F'Trainable params: {sum([ p.numel() for p in model.parameters() if p.requires_grad ])/float(1e6):.2f}M')
    model = model.to(G_DEVICE)

    # optimizer
    optimizer = torch.optim.SGD(model.parameters(), lr = config.training.optimizer.SGD.lr)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.training.scheduler.gamma)

    for e in range(config.training.num_epochs):
        # training
        logging.info(F'Train[{e}] started ...')
        train_loss = 0.0
        train_utts = 0
        train_frames = 0
        for b, batch in enumerate(train_dataloader):
            optimizer.zero_grad()

            samples, X, L, X_lens, L_lens = batch
            num_utts = X.shape[0]
            num_frames = X_lens.sum()

            X, L, X_lens, L_lens = X.to(G_DEVICE), L.to(G_DEVICE), X_lens.to(G_DEVICE), L_lens.to(G_DEVICE)
            Y = model(X)  # Y
            loss = Loss(Y, L, X_lens, L_lens)

            utt_loss = loss / num_utts
            frame_loss = loss / num_frames

            if b != 0 and b % config.training.log_interval == 0:
                logging.info(F'  {e}.{b} loss={train_loss / train_utts:.2f}')

            utt_loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_utts += num_utts
            train_frames += num_frames
        logging.info(F'TRAIN[{e}] done. average_loss={(train_loss / train_utts):.2f}')
        scheduler.step()
        print(scheduler.get_last_lr())

        checkpoint_dir = os.path.join('checkpoints')
        os.makedirs(os.path.join(checkpoint_dir), exist_ok = True)
        checkpoint = os.path.join(checkpoint_dir, F'{e}.pt')
        torch.save(model.state_dict(), checkpoint)

        # validation
        logging.info(F'VALID[{e}] started ...')
        valid_loss = 0.0
        valid_utts = 0
        valid_frames = 0
        with torch.no_grad():
            for b, batch in enumerate(valid_dataloader):
                samples, X, L, X_lens, L_lens = batch
                num_utts = len(samples)
                num_frames = X_lens.sum()

                X, L, X_lens, L_lens = X.to(G_DEVICE), L.to(G_DEVICE), X_lens.to(G_DEVICE), L_lens.to(G_DEVICE)
                Y = model(X)
                loss = Loss(Y, L, X_lens, L_lens)

                best_paths, b = CtcGreedySearch(Y, X_lens)
                print(best_paths, b)
                for p in best_paths:
                    print(tokenizer.tokenizer.DecodeIds(p))
                
                valid_loss += loss.item()
                valid_utts += num_utts
                valid_frames += num_frames
        logging.info(F'VALID[{e}] done. average_loss={(valid_loss / valid_utts):.2f}')

        with torch.no_grad():
            test_model = Model(config.model)
            test_model.load_state_dict(torch.load(checkpoint))


def test():
    pass


if __name__ == '__main__':
    run()
    #test()
