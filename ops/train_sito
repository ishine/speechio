#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.
#

import os, sys
import argparse
import csv
from typing import Optional, Any
from dataclasses import dataclass
import logging.config
import math

from omegaconf import OmegaConf
import yaml
import sentencepiece as spm

import pandas as pd

import numpy
import torch
import torch.nn as nn
import torchaudio
# torchaudio.set_audio_backend('sox_io')

import k2

# Global Constants
G_INT16MAX = 32768
G_EPSILON = 1e-10
G_LOG_EPSILON = math.log(G_EPSILON)
G_FEATURE_PADDING_VALUE = float(0.0)
G_LABEL_PADDING_VALUE = int(-1)
G_16BIT_PCM_MAX = 1 << 15

G_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

@dataclass
class Sample:
    id: str = ''
    audio: str = ''
    begin: float = 0.0
    duration: float = 0.0
    text: str = ''
    speaker: Any = None


class SampleLoader:
    ''' A callable wrapper class to load one dataset sample from one utterance
    input:
        utt dict is typically yielded by .tsv .jsonl .yaml file reader.

    return:
        a Sample object or None
        None if an utterance doesn't satisfy constraints e.g. duration, text length
    '''
    def __init__(self, config):
        self.config = config
    
    def __call__(self, utt: dict, dataset_root: str = '') -> Optional[Sample] :
        sample = Sample()
        for f, attr in self.config.field_map.items():
            assert hasattr(sample, f), F'field:{f} <- attr:{attr}, no such field'
            v = utt.get(attr, None)
            if v:
                if f in ['duration', 'begin', ]:
                    v = float(v)
                elif f == 'audio':
                    v = os.path.join(dataset_root, v)
                else:
                    v = v
                setattr(sample, f, v)

        if float(sample.duration) < self.config.min_duration:
            return None
        if float(sample.duration) > self.config.max_duration:
            return None

        return sample


class Dataset:
    def __init__(self, config_dataset, sample_loader: callable) :
        self.samples = []

        for subset in config_dataset.subsets:
            if subset.max_num_samples <= 0:
                continue

            logging.info(F'Loading dataset: {subset.id} <- ({subset.root}, {subset.metadata})')
            n = 0
            with open(subset.metadata, 'r', encoding='utf8') as f:
                if str.endswith(subset.metadata, '.tsv'):
                    utterance_reader = csv.DictReader(f, delimiter='\t')
                else:
                    raise NotImplementedError

                for utt in utterance_reader:
                    sample = sample_loader(utt, dataset_root = subset.root)
                    if sample != None:
                        self.samples.append(sample)
                        n += 1
                    if n >= subset.max_num_samples:
                        break # early stop of current subset loading
            logging.info(F'Loaded {n} samples from {subset.id}')
        
        # length sort

        # shuffle

                    
    
    def __getitem__(self, index: int):
        return self.samples[index]
    
    def __len__(self):
        return len(self.samples)


def ReadAndResampleAudio(sample: Sample, target_sample_rate: int) :
    '''
    https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-i-o

    torchaudio.info(sample.audio)
    -> 
    AudioMetaData(
        sample_rate=16000,
        num_frames=31872,
        num_channels=1,
        bits_per_sample=16,
        encoding=PCM_S,
    )
    '''
    audio_info = torchaudio.info(sample.audio)

    waveform, _ = torchaudio.load(
        sample.audio,
        frame_offset = int(audio_info.sample_rate * sample.begin),
        num_frames   = int(audio_info.sample_rate * sample.duration),
        channels_first = True,
    )

    if audio_info.sample_rate != target_sample_rate:
        waveform = torchaudio.transforms.Resample(audio_info.sample_rate, target_sample_rate)(waveform)
    
    return waveform, target_sample_rate


def ExtractFeature(waveform: torch.Tensor, sample_rate: int, config):
    '''
    kaldi:
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/mel-computations.h#L60
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/feature-fbank.h#L62

    torchaudio:
        https://pytorch.org/audio/stable/compliance.kaldi.html
    '''
    waveform *= G_16BIT_PCM_MAX
    return torchaudio.compliance.kaldi.fbank(
        waveform,
        sample_frequency = sample_rate,
        num_mel_bins = config.num_mel_bins,
        dither = config.dither,
        energy_floor = G_EPSILON,
        # about energy_floor:
        #   Kaldi default = 0.0; torchaudio default = 1.0;
        #   it doesn't matter if use_energy = False (default in both)
    )


class Tokenizer:
    def __init__(self, config):
        self.tokenizer = spm.SentencePieceProcessor()
        self.tokenizer.load(config.model)
        self.vocab = [ [ self.tokenizer.id_to_piece(x), x] for x in range(self.tokenizer.get_piece_size())]
    
    def __call__(self, text) -> tuple[list[str], list[int]] :
        token_pieces = self.tokenizer.EncodeAsPieces(text)
        token_ids = self.tokenizer.EncodeAsIds(text)
        return token_pieces, token_ids


class DataPipeline:
    def __init__(self, config, tokenizer: Optional[callable] = None, text_normalizer: Optional[callable] = None, mean_norm_shift: Optional[torch.Tensor] = None, var_norm_scale: Optional[torch.Tensor] = None):
        self.config = config

        self.text_normalizer = text_normalizer
        self.tokenizer = tokenizer

        self.mean_norm_shift = mean_norm_shift
        self.var_norm_scale = var_norm_scale

    def __call__(self, samples: list[Sample]):
        features = []
        labels = []
        minibatch = []
        for sample in samples:
            # raw audio loading and resampling
            waveform, sample_rate = ReadAndResampleAudio(sample, target_sample_rate = self.config.model_sample_rate)

            # speed perturbation

            # volumn perturbation

            # reverb

            # add noise
            
            # feature extraction
            feature = ExtractFeature(waveform, sample_rate, self.config.feature).detach()

            # specaug

            # mean normalization
            if (self.mean_norm_shift != None):
                feature += self.mean_norm_shift

            # variance normalizaton
            if (self.var_norm_scale != None):
                feature *= self.var_norm_scale

            features.append(feature)

            text = sample.text
            # text normalization
            if self.text_normalizer:
                text = self.text_normalizer(text)

            # tokenization
            token_pieces, token_ids = [], []
            if self.tokenizer:
                token_pieces, token_ids = self.tokenizer(text)

            labels.append(token_ids)
            
            # store everything here for debug purpose
            minibatch.append(
                (sample.id, waveform, sample_rate, feature, text, token_pieces, token_ids)
            )
        
        # sequence padding & minibatch packing
        # X dims: [n, max(t), f]
        X = torch.nn.utils.rnn.pad_sequence(features, batch_first = True, padding_value = G_FEATURE_PADDING_VALUE)
        X_lens = torch.tensor([ len(x) for x in features ])

        # L dims: [b, max(u)]
        L = torch.nn.utils.rnn.pad_sequence([ torch.tensor(l) for l in labels ], batch_first = True, padding_value = G_LABEL_PADDING_VALUE)
        L_lens = torch.tensor([ len(l) for l in labels ])

        return minibatch, X, L, X_lens, L_lens


class Model(torch.nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.nnet = nn.Sequential(
            nn.Linear(config.idim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.hidden_layer_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_layer_dim, config.odim),
            nn.LogSoftmax(dim = 2),
        )
    
    def forward(self, x):
        y = self.nnet(x)
        return y


def Loss(log_probs, targets, input_lens, target_lens, blank_index = 0):
    log_probs = torch.einsum("btv->tbv", log_probs)
    loss = torch.nn.functional.ctc_loss(
        log_probs,
        targets,
        input_lens,
        target_lens,
        blank_index,
        zero_infinity=True,
        reduction='sum', # this will yield batch-sum loss
    )
    return loss


def ComputeMvnNormalizer(dataset, config):
    pipeline = DataPipeline(config)

    # data loaders
    dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle = config.dataloader.shuffle,
        batch_size = config.dataloader.batch_size, 
        drop_last = config.dataloader.drop_last,
        num_workers = config.dataloader.num_workers,
        collate_fn = pipeline,
    )

    logging.info('Computing global mean variance stats')
    M = torch.zeros(config.model.idim)
    V = torch.zeros(config.model.idim)
    k = 0
    for minibatch in dataloader:
        samples, _,_,_,_ = minibatch
        for sample in samples:
            feat = sample[3]
            M += feat.sum(dim=0)
            V += feat.square().sum(dim=0)
            k += feat.shape[0]
    M = M / k
    V = (V / k - M.square())
    mean_norm_shift = -M
    var_norm_scale = V.sqrt().clamp(min = G_EPSILON).reciprocal()
    logging.info(F'Global MEAN/VAR Normalizer after {k} frames:\nmean_norm_shift:  {mean_norm_shift}\nvar_norm_scale:  {var_norm_scale}')
    return mean_norm_shift, var_norm_scale


def DumpTensorToCSV(x: torch.Tensor, filename: str):
    df = pd.DataFrame(x.numpy())
    df.to_csv(filename)


def run():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type = str, required = True)
    args = parser.parse_args()

    config = OmegaConf.load(args.config)
    print(OmegaConf.to_yaml(config), file = sys.stderr, flush = True)

    # setup logging
    if os.path.isfile(config.logging):
        with open(config.logging, 'r', encoding='utf8') as f:
            logging.config.dictConfig(yaml.safe_load(f))
    else:
        logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    # data sets
    sample_loader = SampleLoader(config.sample_loader)
    train_dataset = Dataset(config.dataset.train, sample_loader = sample_loader)
    valid_dataset = Dataset(config.dataset.valid, sample_loader = sample_loader)

    # data pipline
    mean_norm_shift, var_norm_scale = ComputeMvnNormalizer(train_dataset, config)
    torch.save(mean_norm_shift, 'mean_norm_shift.pt')
    torch.save(var_norm_scale, 'var_norm_scale.pt')

    tokenizer = Tokenizer(config.tokenizer)
    data_pipeline = DataPipeline(config, tokenizer = tokenizer, mean_norm_shift = mean_norm_shift, var_norm_scale = var_norm_scale)

    # data loaders
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, 
        shuffle = config.dataloader.shuffle,
        batch_size = config.dataloader.batch_size, 
        drop_last = config.dataloader.drop_last,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    valid_dataloader = torch.utils.data.DataLoader(
        valid_dataset, 
        shuffle = False,
        batch_size = config.dataloader.batch_size, 
        drop_last = False,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    # model
    model = Model(config.model)
    logging.info(F'Total params:     {sum([ p.numel() for p in model.parameters()                    ])/float(1e6):.2f}M')
    logging.info(F'Trainable params: {sum([ p.numel() for p in model.parameters() if p.requires_grad ])/float(1e6):.2f}M')
    model = model.to(G_DEVICE)

    # optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr = config.training.optimizer.Adam.lr)

    for e in range(config.training.num_epochs):
        # training
        logging.info(F'Train[{e}] started ...')
        train_loss = 0.0
        train_utts = 0
        for b, batch in enumerate(train_dataloader):
            samples, X, L, X_lens, L_lens = batch
            batch_size = len(samples)
            #print(samples)

            X, L, X_lens, L_lens = X.to(G_DEVICE), L.to(G_DEVICE), X_lens.to(G_DEVICE), L_lens.to(G_DEVICE)
            Y = model(X)
            batch_loss = Loss(Y, L, X_lens, L_lens)
            loss = batch_loss / batch_size
            logging.info(F'  {e}.{b} loss={loss:.2f}')

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            # scheduler.step()

            train_loss += batch_loss.item()
            train_utts += batch_size
        logging.info(F'TRAIN[{e}] Done with average_loss={(train_loss / train_utts):.2f}')

        checkpoint_dir = os.path.join('checkpoints')
        os.makedirs(os.path.join(checkpoint_dir), exist_ok = True)
        checkpoint = os.path.join(checkpoint_dir, F'{e}.pt')
        torch.save(model.state_dict(), checkpoint)

        # validation
        logging.info(F'VALID[{e}] started ...')
        valid_loss = 0.0
        valid_utts = 0
        with torch.no_grad():
            for b, batch in enumerate(valid_dataloader):
                samples, X, L, X_lens, L_lens = batch
                batch_size = len(samples)

                X, L, X_lens, L_lens = X.to(G_DEVICE), L.to(G_DEVICE), X_lens.to(G_DEVICE), L_lens.to(G_DEVICE)
                Y = model(X)
                batch_loss = Loss(Y, L, X_lens, L_lens)
                
                valid_loss += batch_loss.item()
                valid_utts += batch_size
        logging.info(F'VALID[{e}] Done average_loss={(valid_loss / valid_utts):.2f}')

        with torch.no_grad():
            test_model = Model(config.model)
            test_model.load_state_dict(torch.load(checkpoint))


def test():
    pass


if __name__ == '__main__':
    run()
    #test()
