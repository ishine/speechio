#!/usr/bin/env python3
# coding = utf8
#
# Copyright (c) 2021 Jiayu DU
# All rights reserved.
#

import os, sys
import argparse
import csv
from typing import Optional, Any
from dataclasses import dataclass
import logging.config

import math
import torch
import torchaudio#; torchaudio.set_audio_backend('sox_io')

from omegaconf import OmegaConf
import yaml
import sentencepiece as spm


# Global Constants
G_INT16MAX = 32768
G_EPSILON = 1e-10
G_LOG_EPSILON = math.log(G_EPSILON)

@dataclass
class Sample:
    id: str = ''
    audio: str = ''
    begin: float = 0.0
    duration: float = 0.0
    text: str = ''
    speaker: Any = None
    audio_info: Any = None  # a place to hold entire-audio info such as codec, bit-width, sample_rate, num_channels etc.


class SampleLoader:
    ''' callable wrapper classï¼š
        Load one dataset sample from a dict with one-utterance info,
        input dict is typically yielded by .tsv .jsonl .yaml file reader.

        return None if an utterance doesn't satisfy certain constraints, e.g. duration too short or too long
    '''
    def __init__(self, config):
        self.config = config
    
    def __call__(self, utt: dict, dataset_root: str = '') -> Optional[Sample] :
        sample = Sample()
        for f, c in self.config.field_map.items():
            assert hasattr(sample, f), F'field:{f} <- column:{c} mapping failed, no such field'
            v = utt.get(c, None)
            if v:
                if f in ['duration', 'begin', ]:
                    v = float(v)
                elif f == 'audio':
                    v = os.path.join(dataset_root, v)
                else:
                    v = v
                setattr(sample, f, v)
        
        # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-i-o
        # torchaudio.info(sample.audio)
        #   -> AudioMetaData(sample_rate=16000, num_frames=31872, num_channels=1, bits_per_sample=16, encoding=PCM_S)
        sample.audio_info = torchaudio.info(sample.audio)
        
        if sample.duration == 0.0:
            entire_audio_duration = float(sample.audio_info.num_frames) / float(sample.audio_info.sample_rate)
            sample.duration = entire_audio_duration

        if float(sample.duration) < self.config.min_duration:
            return None
        if float(sample.duration) > self.config.max_duration:
            return None

        return sample


class Dataset:
    def __init__(self, config_dataset, sample_loader: callable) :
        self.samples = []

        for subset in config_dataset.subsets:
            if subset.max_num_samples <= 0:
                continue
            logging.info(F'Loading dataset: {subset.id} <- ({subset.root}, {subset.metadata})')

            num_subset_samples = 0
            with open(subset.metadata, 'r', encoding='utf8') as f:
                if str.endswith(subset.metadata, '.tsv'):
                    utterance_reader = csv.DictReader(f, delimiter='\t')
                else:
                    raise NotImplementedError

                for utt in utterance_reader:
                    sample = sample_loader(utt, dataset_root = subset.root)
                    if sample != None:
                        self.samples.append(sample)
                        num_subset_samples += 1
                    if num_subset_samples >= subset.max_num_samples:
                        break # early stop of current subset loading

            logging.info(F'Loaded {num_subset_samples} samples from {subset.id}')
                    
    
    def __getitem__(self, index: int):
        return self.samples[index]
    
    def __len__(self):
        return len(self.samples)


def ReadAndResampleAudio(sample: Sample, target_sample_rate: int) :
    waveform, _ = torchaudio.load(
        sample.audio,
        frame_offset = int(sample.audio_info.sample_rate * sample.begin),
        num_frames   = int(sample.audio_info.sample_rate * sample.duration),
        channels_first = True,
    )

    if sample.audio_info.sample_rate != target_sample_rate:
        waveform = torchaudio.transforms.Resample(sample.audio_info.sample_rate, target_sample_rate)(waveform)
    
    return waveform, target_sample_rate


def ExtractFeature(waveform: torch.Tensor, sample_rate: int, config):
    '''
    kaldi:
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/mel-computations.h#L60
        https://github.com/kaldi-asr/kaldi/blob/master/src/feat/feature-fbank.h#L62

    torchaudio:
        https://pytorch.org/audio/stable/compliance.kaldi.html
    '''
    return torchaudio.compliance.kaldi.fbank(
        waveform,
        sample_frequency = sample_rate,
        num_mel_bins = config.num_mel_bins,
        dither = config.dither,
        energy_floor = G_EPSILON, # Kaldi default = 0.0; torchaudio default = 1.0; doesn't matter if use_energy = False (default in both)
    )


class Tokenizer:
    def __init__(self, config):
        self.tokenizer = spm.SentencePieceProcessor()
        self.tokenizer.load(config.model)
    
    def __call__(self, text) -> tuple[list[str], list[int]] :
        token_pieces = self.tokenizer.EncodeAsPieces(text)
        token_ids = self.tokenizer.EncodeAsIds(text)
        return token_pieces, token_ids


class DataPipeline:
    def __init__(self, config, tokenizer: callable, text_normalizer: callable = None):
        self.config = config
        self.text_normalizer = text_normalizer
        self.tokenizer = tokenizer

    def __call__(self, minibatch_samples: list[Sample]):
        minibatch = []
        for sample in minibatch_samples:
            # raw audio loading and resampling
            waveform, sample_rate = ReadAndResampleAudio(sample, target_sample_rate = self.config.model_sample_rate)

            # speed perturbation

            # volumn perturbation

            # reverb

            # add noise
            
            # feature extraction
            feature = ExtractFeature(waveform, sample_rate, self.config.feature)

            # specaug

            text = sample.text
            # text normalization
            if self.text_normalizer:
                text = self.text_normalizer(text)

            # tokenization
            token_pieces, token_ids = self.tokenizer(text)

            minibatch.append(
                (sample.id, waveform, sample_rate, feature, text, token_pieces, token_ids)
            )

        return minibatch


def run():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type = str, required = True)
    args = parser.parse_args()

    config = OmegaConf.load(args.config)
    print(OmegaConf.to_yaml(config), file = sys.stderr, flush = True)

    # setup logging
    if os.path.isfile(config.logging):
        with open(config.logging, 'r', encoding='utf8') as f:
            logging.config.dictConfig(yaml.safe_load(f))
    else:
        logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    # data sets
    sample_loader = SampleLoader(config.sample_loader)
    train_dataset = Dataset(config.dataset.train, sample_loader = sample_loader)

    # data pipline
    tokenizer = Tokenizer(config.tokenizer)
    data_pipeline = DataPipeline(config, tokenizer = tokenizer)

    # data loaders
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, 
        shuffle = config.dataloader.shuffle,
        batch_size = config.dataloader.batch_size, 
        drop_last = config.dataloader.drop_last,
        num_workers = config.dataloader.num_workers,
        collate_fn = data_pipeline,
    )

    # nnet training loop
    for e in range(config.training.num_epochs):
        for b, batch in enumerate(train_dataloader):
            logging.info(F'Epoch={e}, Batch={b}')
            print(batch)


def test():
    pass


if __name__ == '__main__':
    run()
    #test()
