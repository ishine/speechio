#!/usr/bin/env python3
# Copyright (c) 2021 Jiayu DU
# All rights reserved.

import sys, os
import argparse
from contextlib import contextmanager

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--backend', choices = ['sentencepiece'], default = 'sentencepiece')
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--input", type=str, default='-')
    parser.add_argument("--input_format", choices=["piece", "id"], default="piece")
    parser.add_argument("--output", type=str, default='-')
    args = parser.parse_args()

    @contextmanager
    def open_stream(fname, mode):
        if fname == '-':
            if mode == 'r':
                yield sys.stdin
            elif mode == 'w+':
                yield sys.stdout
            else:
                raise NotImplementedError
        else:
            yield open(fname, mode, encoding='utf8')

    if args.backend == 'sentencepiece':
        import sentencepiece as spm

        sp = spm.SentencePieceProcessor()
        sp.Load(args.model + '.model')
        
        with open_stream(args.input, 'r') as istream, open_stream(args.output, 'w+') as ostream:
            for l in istream:
                if args.input_format == "piece":
                    pieces = l.strip().split()
                    decoded = sp.DecodePieces(pieces)
                elif args.input_format == "id":
                    ids = [ int(x) for x in l.strip().split() ]
                    decoded = sp.DecodeIds(ids)
                else:
                    raise NotImplementedError
            
                print(decoded, file = ostream)

    else:
        raise NotImplementedError(F'Unsupported tokenizer backend: {args.backend}')